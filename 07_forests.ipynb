{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4807e94-963f-41df-b5d1-086e3df6c76b",
   "metadata": {
    "id": "a4807e94-963f-41df-b5d1-086e3df6c76b"
   },
   "source": [
    "*updated 11 Aug 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 4303 \"AI and Machine Learning in Marine Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES4303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844859c7-e5a4-4df2-b2dc-b4386bd70af8",
   "metadata": {
    "id": "844859c7-e5a4-4df2-b2dc-b4386bd70af8"
   },
   "source": [
    "---\n",
    "# 7. Random forests and gradient boosting\n",
    "\n",
    "### TL;DR: Basically just ***ensembles*** of decision trees.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vlEyC0x.jpeg\" width=\"500\" alt='forests of randoms'>\n",
    "\n",
    "In that sense this notebook is going to reasonably short and focus more on the technicalities and details. I am only going to do the classification problem; the regression problem and extended investigation is left as an exercise.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Elaborate on the benefits of using an ensemble of decision trees.\n",
    "> 2. Provide the distinctions between the two ensemble methods based around decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928ddb6-de42-4475-8edf-389eb0ff6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# load the penguin data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33787ae-84ab-484f-9dcb-8695b103dfd9",
   "metadata": {
    "id": "b33787ae-84ab-484f-9dcb-8695b103dfd9"
   },
   "source": [
    "---\n",
    "## a) Random forests\n",
    "\n",
    "The idea here is that a single decision tree is prone to over-fitting, and the structure of the tree itself can demonstrate quite a bit of variability in the eventual classification / regression depending on initialisation and/or choice of data. One way to get around this is to train up a collection of trees (i.e. a \"forest\") and take a (possibly weighted) average of the decisions, and call that final product your classifer / regressor. Because of the aveaging operation you might suspect the resulting model to be more robust and less prone to over-fitting.\n",
    "\n",
    "The way this roughly works is as follows the pictorial below (with exaggerations):\n",
    "\n",
    "<img src=\"https://i.imgur.com/bOy1l4N.jpeg\" width=\"1200\" alt='schematic'>\n",
    "\n",
    "### 1) Bootstrap sampling\n",
    "\n",
    "In your training set you create sub-samples of the training data via ***bootstrap sampling***, a bit like when you did $K$-fold cross-validation. The difference here is you allow for replacement of data. As an example, if the whole dataset is $[a, b, c, d, e]$ and I allow sampling four of these to form one of my sub-training sets, then the following are permissiable under bootstrap sampling but not in $K$-fold splitting:\n",
    "\n",
    "* $[a, b, a, c]$\n",
    "* $[a, b, b, a]$\n",
    "* $[a, a, a, a]$\n",
    "\n",
    "Note that $[a, e, c, d]$ would be permissable under both with and without replacement sampling.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> In reality if you were doing $K$-folds you probably wouldn't get $[a, e, c, d]$ as one of the folds anyway, why is that?\n",
    "\n",
    "### 2) Bagging (or boostrap aggregation)\n",
    "\n",
    "***Bagging*** is just you training models based on the dataset obtained from bootstrap aggregation like in the above. The ***random*** part in the \"random forest\" comes from the fact that the bootstrap sampling is going to introduce randomness in the created models.\n",
    "\n",
    "Note the ensemble members are in general going to be weak learners, becasue the exposed sub-sample datasize will likely be low. The idea is that you have multiple weak learners that aggregate into a robust stronger learner. See the [original paper](https://link.springer.com/article/10.1023/A:1018054314350) for why this should work.\n",
    "\n",
    "### 3) Averaging / voting\n",
    "\n",
    "Once you have the ensemble members, then when you use the ensemble to make a prediction, all of the members make a prediction, and the result is averaged for a regressor, or a majority vote is taken for a classifer.\n",
    "\n",
    "Note because this is an ensemble method, some measures of probability in the predictions are in principle given.\n",
    "\n",
    "> NOTE: The things described above can in principle be applied to other methods (e.g. linear models, neural networks), although it is most commonly used with decision trees.\n",
    ">\n",
    "> In `sklearn` the averaging and voting is simple in that the weights are uniform (no biases). You also only get votes (as a probability) in the classifier. Could in principle bully the code to deal with uneven weights, but this is not demonstrated here.\n",
    "\n",
    "Below code demonstrates the usage of random forests with the penguin data, first using the `RandomForestClassifer` object. I am deliberately forcing the resulting trees to be shallow. with `max_depth=2`, and I create 20 trees here (`n_estimators=20`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efbc9a-99e9-4cd6-92c1-7db768a50811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# aim: predict species from features (exactly as in trees basically)\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "\n",
    "# turn target from text to numerical values\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df[\"species\"])\n",
    "\n",
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4167)\n",
    "\n",
    "# use an ensemble of 20 trees (but same max_depth and criterion as before)\n",
    "model = RandomForestClassifier(max_depth=2, n_estimators=20, criterion=\"entropy\",\n",
    "                              random_state=4167)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be098ed-dc2e-4b54-8557-2dbcc1e5c7c5",
   "metadata": {
    "id": "7be098ed-dc2e-4b54-8557-2dbcc1e5c7c5"
   },
   "source": [
    "Note in contrast to the decision tree example in the previous lecture, when I specify `max_depth=2` I end up not using the `body_mass_g` feature. Here the feature importance shows up as a non-zero (but small) value, indicating that some of the ensemble members did pick that feature up to do segmentation on.\n",
    "\n",
    "We can visualise some of the ensemble members below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fa1f6-0beb-463f-839d-35bca191d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise some of trees\n",
    "\n",
    "# randomly choose 4 of the \"n_estimators\" to plot\n",
    "inds = np.arange(20)\n",
    "np.random.shuffle(inds)\n",
    "ind = inds[:4]  # pick out the first four\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i+1)\n",
    "    plot_tree(model.estimators_[ind[i]], filled=True, proportion=True, ax=ax,\n",
    "             feature_names=feature_names, class_names=encoder.classes_)\n",
    "    ax.set_title(f\"tree #{ind[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c9f13-1f74-4462-8d8d-f6a4c7a80399",
   "metadata": {
    "id": "566c9f13-1f74-4462-8d8d-f6a4c7a80399"
   },
   "source": [
    "The `RandomForestClassifier` object comes with probability measures that we can probe, using `model.predict_proba()`. `X_test` should be the input in this case, and the outputs are the probabilities over the ensemble (of size `n_estimators`) of predicting class `0`, `1` or `2` (corresponding to `Adelie`, `Chinstrap` and `Gentoo`).\n",
    "\n",
    "In the below I plot the probabilities, and the classifier returns the predicted label associated with the largest probability. I also label the actual labels by the bars with the brightest bar. Notice then the tallest bars do not correspond to the brightest, i.e. a wrong classification, although it is mostly the case. This observation is consistent with the summary statistics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927d472-3582-481c-bf4a-ab393a7c1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)\n",
    "x = np.arange(len(Y_test))\n",
    "width = 0.2\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "\n",
    "# do a quick plot for legend purposes but never actually show it\n",
    "x_temp = -10\n",
    "for j in range(3):\n",
    "    ax.bar(x_temp+(j-1)*width, probs[0, j], width=width, color=f\"C{j}\",\n",
    "           label=f\"{encoder.classes_[j]}\")\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([10.6, 20.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([40.6, 50.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlabel(\"test data point #\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ee8c3-17eb-414d-a1eb-5e3e8bd44213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64cf51ff-763a-4fe7-b2b0-9fd7232a307f",
   "metadata": {
    "id": "64cf51ff-763a-4fe7-b2b0-9fd7232a307f"
   },
   "source": [
    "### Out-Of-Bag score\n",
    "\n",
    "This is a measure of how we might expect the models to generalise to new data. First, ***out-of-bag*** is the complement of the bagged samples: following example above, if your whole dataset is $[a,b,c,d,e]$, and the bagging gives a subset of $[a, c, b, a]$, then the out-of-bag part is\n",
    "\\begin{equation*}\n",
    "    \\mbox{OOB} = [a,b,c,d,e] \\setminus [a,c,b,a] = [a,b,c,d,e] - [a,b,c] = [d, e],\n",
    "\\end{equation*}\n",
    "where $\\setminus$ is to mean be a set operation and ignoring duplicates.\n",
    "\n",
    "The out-of-bag score is then the average skill of the ensemble members in predicting things out-of-bag. To enable this metric we pass `oob_score=True` in (it is `False` by default because then you do fewer calculations). A value of 1 means the members all predicted with 100% accuracy, and a value closer to 1 would suggest models are likely going to generalise better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbe96c-b2ae-4954-83de-0bff39e9110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an ensemble of 20 trees (but same max_depth and criterion as before)\n",
    "model = RandomForestClassifier(max_depth=2, n_estimators=20, criterion=\"entropy\",\n",
    "                              random_state=4167, oob_score=True)\n",
    "model.fit(X_train, Y_train)\n",
    "print(f\"model OOB accuracy = {model.oob_score_*100:.4f}%\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281043ef-2829-4955-9ed1-a9c7ba6e3ef7",
   "metadata": {
    "id": "281043ef-2829-4955-9ed1-a9c7ba6e3ef7"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> For me the problematic test set data seems to be at index 13, 41 and 42. Have a look and those associated data points and see if there is a reason they are particularly hard for the prediction step (e.g. are they in a region that is particularly \"mixed\"?)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try some of the above for `RandomForestRegressor`, e.g. do the prediction problem I did last time. The only thing you don't have access to are the probabilities of the predictions (the returned value is an average of all ensemble members).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Consider changing other things such as pruning parameter `ccp_alpha`, `max_depth`, `min_samples_leaf`, `criterion` and see how performance differs. Could also do cross-validation and hyper-parameter tuning accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f61c80-e940-4544-81a4-380e58e1a87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b19ace93-238e-4163-bc34-c42387c69f01",
   "metadata": {
    "id": "b19ace93-238e-4163-bc34-c42387c69f01"
   },
   "source": [
    "---\n",
    "## b) Gradient boosting\n",
    "\n",
    "The idea of ensemble methods is that you train a whole load of weak models, but the variance of the models are reduced through some sort of averaging procedure, leading to ensemble predictions that are more robust, but it may or may not say anything about the bias. The idea of ***boosting*** is to target the bias by targeting the problematic predictions, and considers the following pipeline:\n",
    "\n",
    "1. Train an ensemble of models again, and identify the particularly weak models, i.e. the ones that got misclassified or had particularly large prediction errors.\n",
    "2. Take those biases and compute residuals/mismatches/losses, change the weighting of the DATA to improve those particularly weak models, with the aim to \"boost\" the performance of the overall ensemble.\n",
    "5. Re-train the model, repeat the processing of weight updates, and iterate until you get some convergence of the overall ensemble.\n",
    "\n",
    "In that sense it's an optimisation problem (again!), hence my usage of familiar terms. The ***gradient*** part is that it uses (stochastic) gradient descent type methods to solve the optimisation problem associated with ***boosting***.\n",
    "\n",
    "The demonstration below applies `GradientBoostingClassifier` to the same problem as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9242602-db0a-4b58-bfce-8f7e1fe6756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# use an ensemble of 20 trees (but same max_depth and criterion as before)\n",
    "model = GradientBoostingClassifier(max_depth=2, n_estimators=20,\n",
    "                                   random_state=4167)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83c985-05da-4b21-9468-921517f60fed",
   "metadata": {
    "id": "cf83c985-05da-4b21-9468-921517f60fed"
   },
   "source": [
    "So in this case with exactly the basically the same settings as the `RandomForestClassifer` the skill on the test set is comparable to the random forests. What is slightly different are the associated probabilities in the resulting trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8387f0-148a-4d8f-890b-a98e46ca7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the associated probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "x = np.arange(len(Y_test))\n",
    "width = 0.2\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "\n",
    "# do a quick plot for legend purposes but never actually show it\n",
    "x_temp = -10\n",
    "for j in range(3):\n",
    "    ax.bar(x_temp+(j-1)*width, probs[0, j], width=width, color=f\"C{j}\",\n",
    "           label=f\"{encoder.classes_[j]}\")\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([10.6, 20.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([40.6, 50.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlabel(\"test data point #\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722418b-f450-4f2a-9328-eae267e1bb76",
   "metadata": {
    "id": "2722418b-f450-4f2a-9328-eae267e1bb76"
   },
   "source": [
    "The thing that is probably true in this case is that where the bars were already tall in the random forest case it is even taller here.\n",
    "\n",
    "Interestingly, both classifers are failing at the same prediction points (for me it's index 13, 41 and 42). It is perhaps of interest to have a look at what is going on there.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> For me the problematic test set data seems to be at index 13, 41 and 42. Have a look and those associated data points and see if there is a reason they are particularly hard for the prediction step (e.g. are they in a region that is particularly \"mixed\"?)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try some of the above for `GradientBoostingRegressor`, e.g. do the prediction problem I did last time. The only thing you don't have access to are the probabilities of the predictions (the returned value is an average of all ensemble members).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Consider changing other things such as pruning parameter `ccp_alpha`, `max_depth`, `min_samples_leaf`, `criterion` and see how performance differs. Could also do cross-validation and hyper-parameter tuning accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ddaa08-df90-4bf5-bb6f-f67b8e03b809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c19b1dd-79a7-4bdf-bb46-961902768f32",
   "metadata": {
    "id": "1c19b1dd-79a7-4bdf-bb46-961902768f32"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Regression problems\n",
    "\n",
    "As indicated already.\n",
    "\n",
    "There might be something to be said about doing an even simpler regression problem (e.g. $f(x) = \\sin x + \\epsilon$) and visualising how that works, particularly for gradient boosting as the iterations progress (if possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cc2ae-2ab9-4eea-aa14-97fd9a85c324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0914ad7-13d5-4165-961b-32b57629a136",
   "metadata": {
    "id": "c0914ad7-13d5-4165-961b-32b57629a136"
   },
   "source": [
    "## 2) Image classification and/or extrapolation\n",
    "\n",
    "This is a hard problem. Going to the raw images of cats and/or dogs, consider training\n",
    "\n",
    "1. Classifiers for cats and dogs,\n",
    "2. Regressors on half the face and using it to predict the other half. Analyse the skill on training data, testing data, the need for standardising the data, analysis of the model coefficients, dependence on the propotion of face exposed to model, cross-validation and etc.\n",
    "\n",
    "From a coding point of view it is potentially easier to use the left half of the face to predict the right half (idea below). You can try top and bottom also.\n",
    "\n",
    "Just be aware if you use the full image it might be slow (I never managed to get `GradientBoostingRegressor` to converge for the regression problem, although you could try `HistGradientBoostingRegressor` instead). You may or may not want to do dimension reduction on these first, which will probably help speed up the classification problem quite a bit; not sure about the regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3de42-3115-4a56-ac5d-0190aa75f8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e7198ad-aa08-48ee-b0b4-0d9264044489",
   "metadata": {
    "id": "3e7198ad-aa08-48ee-b0b4-0d9264044489"
   },
   "source": [
    "## 3) Time series forecasting\n",
    "\n",
    "Consider regression on time series data like `elnino34_sst.data`, Lotka-Volterra, Lorenz, or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a6092-ce22-4223-923d-0f79006532e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "FCn7c4nVmYPS",
   "metadata": {
    "id": "FCn7c4nVmYPS"
   },
   "source": [
    "## 4) Extreme Gradient Boosting with XGBoost\n",
    "\n",
    "<!-- JL -->\n",
    "\n",
    "One significant advancement in machine learning is the advent of extreme gradient boosting. Look up what extreme gradient boosting means and what is it doing. The Python package `xgboost` has gained a lot of popularity. See if you can use `xgboost` for the penguins data. Feel free to read the documentation [here](https://xgboost.readthedocs.io/en/stable/get_started.html).\n",
    "\n",
    "Google Colab has `xgboost` installed by default. It is highly encouraged to train the model on Google Colab. Though training could be slower, this is to protect the precious computer of yours. **Also, be very careful with hyperparameters (`n_estimators`, `max_depth` etc.)! Control your model wisely and do not go insane with parameters!**\n",
    "\n",
    "Notice any advantages / disadventages of `xgboost`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eDzfCsIoA2m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "# from xgboost import XGBClassifier\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
