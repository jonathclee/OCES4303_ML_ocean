{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a1ea43-8419-4637-8f35-9e6e02235510",
   "metadata": {
    "id": "c5a1ea43-8419-4637-8f35-9e6e02235510"
   },
   "source": [
    "*updated 03 Aug 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 4303 \"AI and Machine Learning in Marine Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES4303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0427fc0-070b-4b6c-bb82-ac44349d606d",
   "metadata": {
    "id": "a0427fc0-070b-4b6c-bb82-ac44349d606d"
   },
   "source": [
    "---\n",
    "# 4. Clustering\n",
    "\n",
    "Given unlabelled data it may be of interest to label them according to how \"close\" the data is to each other. ***Clustering*** is a means to do this: you group and label blocks of data samples according to some criterion related to whatever you want to mean by \"closeness\".\n",
    "\n",
    "One reason you may want to have different regression models or train regression models differently depending on the data. Piggy-backing on some previous material:\n",
    "\n",
    "1. From the penguins example you may remember that Gentoo has somewhat different properties compared to the other two. Models trained on everything is not expected to work that well (and it doesn't). In that case it is obvious to split by `species` (because that already comes with the data), but you could imagine you got unlabelled data instead. Then you'd want to cluster first and then possibly train different models accordingly.\n",
    "2. You may want to reduce dimension and ideally separate out data, then cluster, then train models accordingly. The labels also provide extra features that could be fed to the model.\n",
    "\n",
    "> NOTE: The above points are things you should probably consider doing certainly for the first assignment relating to Argo data.\n",
    "\n",
    "Clustering algorithms work by having some measure of \"closeness\", whatever you want that to mean. Going to demonstrate roughly how clustering algorithms might work and some subtleties related to the measure of \"closeness\".\n",
    "\n",
    "> NOTE: This is again a problem in the choice of \"metric\".\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Understand the rough idea and possible uses of clustering.\n",
    "> 2. Note how clustering depends on the measure of distance, and how that plays with dimension reduction techniques.\n",
    "> 3. Implement and explore the use of clustering for oceanic applications (see assignment also)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536df115-1772-4351-bedd-6e140f797565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05153b22-0c0a-4a35-bfd5-7d5e9de122cf",
   "metadata": {
    "id": "05153b22-0c0a-4a35-bfd5-7d5e9de122cf"
   },
   "source": [
    "---\n",
    "## a) $K$-means clustering\n",
    "\n",
    "A simple case for clustering that we can demonstrate reasonably easily is ***$K$-means clustering***. I am going to demonstrate how this works in 2d manually first, and the generalisation to arbitrary dimensions and use in `sklearn` should be fairly obvious.\n",
    "\n",
    "### 0. Create some data first\n",
    "\n",
    "Below I am going to artificially create three blobs of data (and kicked around by some noise). By eye it should be suggestive that there would be three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a7569-a30f-4c3e-92a2-0da2585c1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cluster centers (in a triangle in this case), then add noise to them\n",
    "X = np.concat((np.ones((30, 2)) * [ 0.0, 1.0],\n",
    "               np.ones((30, 2)) * [ 1.0, 0.0],\n",
    "               np.ones((30, 2)) * [-1.0, 0.0]))\n",
    "X += 0.25 * np.random.randn(X.shape[0], 2)  # made this small-ish so clusters show up\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(X[:, 0], X[:, 1], 'x')\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e4fd6-af42-4271-bb61-7abd9e8eb4e9",
   "metadata": {
    "id": "195e4fd6-af42-4271-bb61-7abd9e8eb4e9"
   },
   "source": [
    "### 1. Specify number of clusters $K$ and location of their initial centroid\n",
    "\n",
    "A user defined parameter (and thus a model hyper-parameter) is the number of clusters $K$ (hence $K$-means). Step 0 of plotting out the data suggests to me maybe I should choose $K = 3$, so I do that and in this case take an educated guess where the centres of the clusters (the **centroid**) are; we will consider the randomly initialised case later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3892a1d-6e4f-43e2-a1b6-336bc2a46f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise centroid guesses and plot where these are\n",
    "centroid = np.array([[0.0, 0.5], [0.5, 0.2], [-0.5, -0.2]]) # centroid[number, coord]\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(X[:, 0], X[:, 1], 'x')\n",
    "for i in range(centroid.shape[0]):\n",
    "    ax.plot(centroid[i, 0], centroid[i, 1], 's', markersize=10, label=f\"{i}\")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9641e5-edf9-4f08-b24b-26e33ae757ae",
   "metadata": {
    "id": "8d9641e5-edf9-4f08-b24b-26e33ae757ae"
   },
   "source": [
    "### 2. Label the clusters according to how close they are to the centroids\n",
    "\n",
    "This depends on the choice of distance, and the usual (but not the only) choice is to take the $L^2$ distance (sometimes called ***Euclidean distance***). Below I do just that although I ignore the square root procedure: if you had units you would need a square root to get a distance, but the magnitude ordering is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e7dd5-f861-4a1d-b386-8b9f00d16d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate by computing distances and assigning labels to points closest to centroids\n",
    "\n",
    "# L2 or root-mean-square distance (the root is not actually important here)\n",
    "dist = np.sum((X - centroid[0, :]) ** 2, axis=-1)  # sum along the co-ord dimension\n",
    "label = np.zeros(X.shape[0])  # set everything to belong to zero for now\n",
    "\n",
    "for i in range(1, centroid.shape[0]):\n",
    "    # compute new distance, and update baseline references as appropriate\n",
    "    dist_dum = np.sum((X - centroid[i, :]) ** 2, axis=-1)\n",
    "    label = np.where(dist_dum < dist, i, label)\n",
    "    dist = np.where(dist_dum < dist, dist_dum, dist)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "for i in range(centroid.shape[0]):\n",
    "    ax.plot(X[label==i, 0], X[label==i, 1], f\"C{i+1}x\")\n",
    "    ax.plot(centroid[i, 0], centroid[i, 1], f\"C{i+1}s\", markersize=10, label=f\"{i}\")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bd15a-b7d3-407e-9120-855d7d661d5a",
   "metadata": {
    "id": "cf5bd15a-b7d3-407e-9120-855d7d661d5a"
   },
   "source": [
    "### 3. Update the centroid locations, and iterate\n",
    "\n",
    "Give the identified clusters, find the \"centres\" of those. I found it below by doing an optimistion problem (find the $L^2$ minimiser), although I am thinking you could probably just take an average (although something doesn't feel right with the procedre to me...)\n",
    "\n",
    "Then you iterate until some tolerance has been reached (e.g. the centroids don't move that much after a certain point, the classification hasn't changed after enough iterations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c8d2e-9f58-4bc1-a54e-9b0dd38dc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new centroid location from classification solving an optimisation problem\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# define the cost function\n",
    "def func(centroid, X):\n",
    "    return np.sum((X - centroid)**2)  # want the L2 minimiser\n",
    "\n",
    "# cycle through the data and update the centroids\n",
    "for i in range(centroid.shape[0]):\n",
    "    res = minimize(func, centroid[i, :], args=X[label==i, :])\n",
    "    centroid[i, :] = res.x\n",
    "\n",
    "# plot new locations\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "for i in range(centroid.shape[0]):\n",
    "    ax.plot(X[label==i, 0], X[label==i, 1], f\"C{i+1}x\")\n",
    "    ax.plot(centroid[i, 0], centroid[i, 1], f\"C{i+1}s\", markersize=10, label=f\"{i}\")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f35dfd-cc74-4de8-bbbf-46439ab2e01c",
   "metadata": {
    "id": "d1f35dfd-cc74-4de8-bbbf-46439ab2e01c"
   },
   "source": [
    "### Doing this through `sklearn`\n",
    "\n",
    "A good exercise is to try writing your own $K$-means (mine wraps all of the above in about 30 lines of code). Failing that, we could use `sklearn`. Below code cells demonstrate the relevant syntax with the artificial data above and considers varying the initial guess via specifying `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e5842-01db-4f80-bff2-50403647fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "random_states = [1, 2, 4, 7]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "for j in range(len(random_states)):\n",
    "    # initialise model, fit and then return labels\n",
    "    model = KMeans(n_clusters=3, random_state=random_states[j])\n",
    "    model.fit(X)\n",
    "    label = model.predict(X)  # could have used \"model.fit_predict\"\n",
    "\n",
    "    ax = plt.subplot(2, 2, j+1)\n",
    "    for i in range(model.n_clusters):\n",
    "        ax.plot(X[label==i, 0], X[label==i, 1], f\"C{i+1}x\")\n",
    "        ax.plot(model.cluster_centers_[i, 0], model.cluster_centers_[i, 1], f\"C{i+1}s\",\n",
    "                markersize=10, label=f\"{i}\")\n",
    "    if j > 1:\n",
    "        ax.set_xlabel(r\"$x$\")\n",
    "    if j % 2 == 0:\n",
    "        ax.set_ylabel(r\"$y$\");\n",
    "\n",
    "    ax.set_title(f\"random state = {random_states[j]}\")\n",
    "    ax.grid()\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10156596-812f-4b78-91ef-4ed8fdc57a71",
   "metadata": {
    "id": "10156596-812f-4b78-91ef-4ed8fdc57a71"
   },
   "source": [
    "The thing to note here is that the clusters themselves are fairly robust although the labels differ. That's a mild annoyance if wanting to use this to compare with already labelled data, but a relabelling with fix that if need be.\n",
    "\n",
    "The algorithm generalises to high dimensions if you think of a piece of data as some point in some high dimensional space with co-ordinates given by each of the numbers in the features dimenison, and taking the higher dimension analogue of the $L^2$ distance.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Have a think what would happen to the classification if two data points happen to have identical distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789dffed-982f-42b7-97cc-cb3df5171ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0e35f6-3c31-403c-851b-773bf963b705",
   "metadata": {
    "id": "2f0e35f6-3c31-403c-851b-773bf963b705"
   },
   "source": [
    "---\n",
    "## b) Manifold methods revisited\n",
    "\n",
    "As noted one needs to then be a bit careful about the choice of distance. In the below example I have two half moons in 2d and an swiss-roll shaped data in 3d, using a tool already in `sklearn` (and is nicer than the examples I manually tried to generate), and I am going to apply $K$-means to it as usual.\n",
    "\n",
    "> NOTE: The `make_s_curve` one is a good choice for 3d testing too.\n",
    "\n",
    "<img src=\"https://i.imgur.com/rOgacgN.png\" width=\"450\" alt='moon and swiss roll'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ad28c-3c8d-43d8-bfc8-167e9901871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_swiss_roll\n",
    "\n",
    "n_samples = 600\n",
    "X_moon, _ = make_moons(n_samples, noise=0.05)     # don't need labels\n",
    "X_swiss, swiss_color = make_swiss_roll(n_samples, noise=0.01)  # don't need swiss_color just yet\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# moon data should have 2 clusters\n",
    "ax = plt.subplot2grid((1, 3), (0, 0))\n",
    "label = KMeans(n_clusters=2).fit_predict(X_moon)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "# S data is one continuous thing, 3 clusters for now\n",
    "ax = plt.subplot2grid((1, 3), (0, 1), colspan=2, projection=\"3d\")\n",
    "label = KMeans(n_clusters=3, random_state=0).fit_predict(X_swiss)\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss[label==i, 0],\n",
    "            X_swiss[label==i, 1],\n",
    "            X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.view_init(azim=-75, elev=9)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "ax.set_anchor(\"W\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628858c-b3a8-437c-af47-bd551d920068",
   "metadata": {
    "id": "3628858c-b3a8-437c-af47-bd551d920068"
   },
   "source": [
    "Both of these demonstrate cases where in this case the ambient measure of distance (in this case $L^2$ in $\\mathbb{R}^2$ and $\\mathbb{R}^3$) is not the relevant distance: the data has a distribution that lives on some surface (which I are going to label as the ***manifold***), and it's the ***intrinsic*** measure of distance on the manifold that should be the relevant one, rather than the ***extrinsic*** one inherited from the data manifold being embedded in this case in and $\\mathbb{R}^2$ and $\\mathbb{R}^3$. One could argue for example that:\n",
    "\n",
    "* The two moons are really 1d curves living in $\\mathbb{R}^2$ up to noise\n",
    "* The Swiss roll is really a 2d surface living in $\\mathbb{R}^3$ up to noise\n",
    "\n",
    "This is where the previous dimensional reduction techniques may be useful: you take the data and try and find a projection onto lower dimension space to pull out the key features (cf. a co-ordinate \"transformation\"), then in the transformed space your \"distance\" may actually make sense, then you do clustering on that.\n",
    "\n",
    "Below demonstrates that with the moon data first, where we expect 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d31786-930a-4d30-a546-6f2ceb913d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding, TSNE\n",
    "\n",
    "# find 1d projections of 2d data\n",
    "lle = LocallyLinearEmbedding(n_components=1, random_state=777)\n",
    "tsne = TSNE(n_components=1, random_state=777)\n",
    "X_moon_lle = lle.fit_transform(X_moon)\n",
    "X_moon_tsne = tsne.fit_transform(X_moon)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# predicting after LLE transformation\n",
    "label = KMeans(n_clusters=2).fit_predict(X_moon_lle)\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon_lle[label==i, 0], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "# predicting after TSNE transformation\n",
    "label = KMeans(n_clusters=2).fit_predict(X_moon_tsne)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon_tsne[label==i, 0], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25920f4c-bda5-4f18-bc09-9e8aa82516de",
   "metadata": {
    "id": "25920f4c-bda5-4f18-bc09-9e8aa82516de"
   },
   "source": [
    "So this one does actually work reasonably well as long as I do a 1d projection, rather than a 2d transformation (which I started off with for some stupid reason in hindsight). The $t$-SNE projection is more robust generally than the LLE one, at least from my empirical testing.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> I find that depending on the initialisation of the data (because the `random_state` is already fixed) the LLE approach is a bit brittle, while the $t$-SNE case is more robust. Consider doing an ensemble of these and test for the robustness.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try a 2d co-ordinate transformation instead. You will need to modify the plotting code somewhat.\n",
    "\n",
    "Below shows an analogous approach on swiss roll data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc78c2-f11f-4749-940e-a7eb7517f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 2d projections (because original data is in 3d)\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, random_state=777)\n",
    "tsne = TSNE(n_components=2, random_state=777)\n",
    "X_swiss_lle = lle.fit_transform(X_swiss)\n",
    "X_swiss_tsne = tsne.fit_transform(X_swiss)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# predicting after LLE transformation\n",
    "label = KMeans(n_clusters=3).fit_predict(X_swiss_lle)\n",
    "ax = plt.subplot2grid((2, 3), (0, 0))\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss_lle[label==i, 0],\n",
    "            X_swiss_lle[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 3), (0, 1), colspan=2, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss[label==i, 0],\n",
    "            X_swiss[label==i, 1],\n",
    "            X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.view_init(azim=-75, elev=9)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "ax.set_anchor(\"W\")\n",
    "\n",
    "# predicting after LLE transformation\n",
    "label = KMeans(n_clusters=3).fit_predict(X_swiss_tsne)\n",
    "ax = plt.subplot2grid((2, 3), (1, 0))\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss_tsne[label==i, 0],\n",
    "            X_swiss_tsne[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 3), (1, 1), colspan=2, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss[label==i, 0],\n",
    "            X_swiss[label==i, 1],\n",
    "            X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.view_init(azim=-75, elev=9)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "ax.set_anchor(\"W\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7259177-b749-4817-be5f-33b079712c2d",
   "metadata": {
    "id": "a7259177-b749-4817-be5f-33b079712c2d"
   },
   "source": [
    "In the swiss roll case it looks like the approaches are able to find segments of the data appropriately, but this is again slightly more brittle depending on initialisations.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Again, consider doing an ensemble of these and test for the robustness.\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "Instead of $K$-means there is an approach called `DBSCAN`, which asserts that clusters occur as high density of data points separated by gaps that are lower density. This is more a local approach and can also be flexible with the choice of metric (hence it's grouping here), although it does have potential drawbacks in that there are more model hyperparameters (`min_samples` and `eps` to give a measure what is meant by \"dense\"); the number of clusters that falls out is a result of those two choices (and thus we have less control on what drops out). Below shows the two demonstrations with the moons and swiss roll data in the original embedding space.\n",
    "\n",
    "> NOTE: The black dots are **noise points** and has a label of `-1`. These are points close enough to multiple identified clusters that DBSCAN is unable to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cbf82-c96a-43c1-b50f-3548bb7aa2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# moon data\n",
    "eps_vec = [0.5, 0.15, 0.05]  # keep default of min_samples = 5\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# moon data should have 2 clusters\n",
    "for j in range(len(eps_vec)):\n",
    "    ax = plt.subplot(1, 3, j+1)\n",
    "    label = DBSCAN(eps=eps_vec[j]).fit(X_moon).labels_\n",
    "    # identify number of labels\n",
    "    n_clusters = len(set(label)) - (1 if -1 in label else 0)\n",
    "    for i in range(n_clusters):\n",
    "        ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "    ax.plot(X_moon[label==-1, 0], X_moon[label==-1, 1], f\"ko\", alpha=0.3)  # noise points\n",
    "    ax.grid()\n",
    "    ax.set_title(f\"$\\epsilon$ = {eps_vec[j]}\")\n",
    "    if j > 0:\n",
    "        ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4b054-3872-4ce2-93ae-299e2466f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swiss roll data\n",
    "eps_vec = [3.5, 2.2, 1.5]  # keep default of min_samples = 5\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# swiss roll data is one continuous cluster\n",
    "for j in range(len(eps_vec)):\n",
    "    ax = plt.subplot(1, 3, j+1, projection=\"3d\")\n",
    "    label = DBSCAN(eps=eps_vec[j]).fit(X_swiss).labels_\n",
    "    # identify number of labels\n",
    "    n_clusters = len(set(label)) - (1 if -1 in label else 0)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        ax.plot(X_swiss[label==i, 0],\n",
    "                X_swiss[label==i, 1],\n",
    "                X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "    ax.plot(X_swiss[label==-1, 0],\n",
    "            X_swiss[label==-1, 1],\n",
    "            X_swiss[label==-1, 2], f\"ko\", alpha=0.3)  # noise points\n",
    "    ax.view_init(azim=-75, elev=9)\n",
    "    ax.set_box_aspect((1, 1, 1))\n",
    "    ax.set_anchor(\"W\")\n",
    "    ax.set_title(f\"$\\epsilon$ = {eps_vec[j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3a338-3d51-4b10-9039-bc57425c7448",
   "metadata": {
    "id": "a2d3a338-3d51-4b10-9039-bc57425c7448"
   },
   "source": [
    "In the moon data if `eps` is too large then it just lumps everything together. If it's too small then there are a lot of noise points and tons of clusters. There is thus an optimum `eps` that gives the expected two.\n",
    "\n",
    "In the swiss roll data, it really is one big cluster so in this case `eps` being large-ish is actually ok. For moderate `eps` it is identifying clusters somewhat according to the unrolling of the swiss roll so that's good. If `eps` is too small there is too much noise as before.\n",
    "\n",
    "So cross-validation and exploration of hyperparameters with DBSCAN is important!\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> I kept `min_samples` as the default value of 5. Explore what happens when you change that.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try the above but also with appropriate dimensional reduction / co-ordinate transformation approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97ab3b-11ab-4624-85a8-614c97a0fbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d85068d4-f849-4227-b342-afd12bde3ecc",
   "metadata": {
    "id": "d85068d4-f849-4227-b342-afd12bde3ecc"
   },
   "source": [
    "---\n",
    "## c) Demonstration: Penguins data\n",
    "\n",
    "We are going to do brute force $K$-means on the penguins data and compare the clustered results with the species ones. Note from the previous lecture that we think the $t$-SNE can do a good job projecting the 4d data down to 2d with a good separation, so we are also going to consider including that as an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349d9ba-7a6b-4632-ae5d-41e96decfc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafe817-9645-4ba3-994f-72cd2358a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out some data\n",
    "X = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values\n",
    "Y = df[\"species\"].values\n",
    "\n",
    "X = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "\n",
    "# instantiate model and fit with all data (no target data is needed since unsupervised)\n",
    "model = KMeans(n_clusters=3, random_state=4)\n",
    "Y_pred = model.fit_predict(X)\n",
    "\n",
    "# do a 3d plot to have a look to see what is going on\n",
    "key1, key2, key3 = \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# predicted clusters\n",
    "ax = plt.subplot(1, 2, 1, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.scatter(X[Y_pred==i, 0],\n",
    "               X[Y_pred==i, 1],\n",
    "               X[Y_pred==i, 2],\n",
    "               label=f\"cluster {i}\",\n",
    "               alpha=0.5)\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)\n",
    "\n",
    "# ground truth\n",
    "ax = plt.subplot(1, 2, 2, projection=\"3d\")\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X[df[\"species\"] == species, 0],\n",
    "               X[df[\"species\"] == species, 1],\n",
    "               X[df[\"species\"] == species, 2],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69805a3-3754-4db7-9abd-f1a2ebf323d5",
   "metadata": {
    "id": "b69805a3-3754-4db7-9abd-f1a2ebf323d5"
   },
   "source": [
    "By eye there is clearly some mismatch going on if you do this brute force. To be more quantitative about it, if we think in this the colours are maching exactly (because I chose my `random_state` deliberately for this), then we can simply take `(Adelie, Gentoo, Chinstrap)` to `(0, 1, 2)`, and see how good the labels match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ce271-9e45-4f74-b43c-ab08996edce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight forward comparison of labels doesn't work, manually doing a remapping\n",
    "Y_remap = np.zeros(Y.shape)\n",
    "Y_remap[Y == \"Adelie\"] = 0\n",
    "Y_remap[Y == \"Gentoo\"] = 1\n",
    "Y_remap[Y == \"Chinstrap\"] = 2\n",
    "\n",
    "N = len(Y_pred)\n",
    "N_0, N_1, N_2 =len(Y_remap[Y_remap==0]), len(Y_remap[Y_remap==1]), len(Y_remap[Y_remap==2])\n",
    "skill_all = np.sum(Y_pred == Y_remap)\n",
    "skill_0   = np.sum(Y_pred[Y_remap==0] == Y_remap[Y_remap==0])\n",
    "skill_1   = np.sum(Y_pred[Y_remap==1] == Y_remap[Y_remap==1])\n",
    "skill_2   = np.sum(Y_pred[Y_remap==2] == Y_remap[Y_remap==2])\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"    cluster 0: {skill_0} correct out of {N_0} ({skill_0/N_0*100:.2f}%)\")\n",
    "print(f\"    cluster 1: {skill_1} correct out of {N_1} ({skill_1/N_1*100:.2f}%)\")\n",
    "print(f\"    cluster 2: {skill_2} correct out of {N_2} ({skill_2/N_2*100:.2f}%)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34f5d8-6d8f-4173-bdbe-9e06a9240ad9",
   "metadata": {
    "id": "ed34f5d8-6d8f-4173-bdbe-9e06a9240ad9"
   },
   "source": [
    "It's actually pretty (it wasn't when I didn't standardise the data). Doing the same but including a $t$-SNE step; for the same choice of `random_state` I need to do a slightly different label remapping here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477eb2ca-180c-4893-9f03-0c4dfca00832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above but with a TSNE step\n",
    "\n",
    "X = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values\n",
    "Y = df[\"species\"].values\n",
    "\n",
    "tsne = TSNE(random_state=4)  # default of 2 components (so 2 dimensions)\n",
    "X = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# instantiate model and fit with all data (no target data is needed since unsupervised)\n",
    "model = KMeans(n_clusters=3, random_state=4)\n",
    "Y_pred = model.fit_predict(X_tsne)\n",
    "\n",
    "# do a 3d plot to have a look to see what is going on\n",
    "key1, key2, key3 = \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# predicted clusters\n",
    "ax = plt.subplot(1, 2, 1, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.scatter(X[Y_pred==i, 0],\n",
    "               X[Y_pred==i, 1],\n",
    "               X[Y_pred==i, 2],\n",
    "               label=f\"cluster {i}\",\n",
    "               alpha=0.5)\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)\n",
    "\n",
    "# ground truth\n",
    "ax = plt.subplot(1, 2, 2, projection=\"3d\")\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X[df[\"species\"] == species, 0],\n",
    "               X[df[\"species\"] == species, 1],\n",
    "               X[df[\"species\"] == species, 2],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95bd54-5ceb-49d0-861f-9007600fbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight forward comparison of labels doesn't work, manually doing a remapping\n",
    "Y_remap = np.zeros(Y.shape)\n",
    "Y_remap[Y == \"Adelie\"] = 2\n",
    "Y_remap[Y == \"Gentoo\"] = 1\n",
    "Y_remap[Y == \"Chinstrap\"] = 0\n",
    "\n",
    "N = len(Y_pred)\n",
    "N_0, N_1, N_2 =len(Y_remap[Y_remap==0]), len(Y_remap[Y_remap==1]), len(Y_remap[Y_remap==2])\n",
    "skill_all = np.sum(Y_pred == Y_remap)\n",
    "skill_0   = np.sum(Y_pred[Y_remap==0] == Y_remap[Y_remap==0])\n",
    "skill_1   = np.sum(Y_pred[Y_remap==1] == Y_remap[Y_remap==1])\n",
    "skill_2   = np.sum(Y_pred[Y_remap==2] == Y_remap[Y_remap==2])\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"    cluster 0: {skill_0} correct out of {N_0} ({skill_0/N_0*100:.2f}%)\")\n",
    "print(f\"    cluster 1: {skill_1} correct out of {N_1} ({skill_1/N_1*100:.2f}%)\")\n",
    "print(f\"    cluster 2: {skill_2} correct out of {N_2} ({skill_2/N_2*100:.2f}%)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gE_txOl5g7dZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JL:\n",
    "# NOTE: Above shows a metric called 'Accuracy'\n",
    "#       There are also other measurement of skills / scores.\n",
    "#       Below shows some of them.\n",
    "# NOTE: There is no explicit answer as to which metric is better,\n",
    "#       but be careful what these metrics represent !!!!\n",
    "#       Some metrics are also not symmetric !!!\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# X = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values\n",
    "# X_scaled = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "\n",
    "# try X or X without standardisation, produce different results\n",
    "# X_tsne will be best becoz it is being properly scaled\n",
    "silhouette = metrics.silhouette_score(X_tsne, Y_pred)\n",
    "db_index = metrics.davies_bouldin_score(X_tsne, Y_pred)\n",
    "ch_index = metrics.calinski_harabasz_score(X_tsne, Y_pred)\n",
    "ari = metrics.adjusted_rand_score(Y_remap, Y_pred)\n",
    "ami = metrics.adjusted_mutual_info_score(Y_remap, Y_pred)\n",
    "\n",
    "# Print the metric scores\n",
    "print(f\"Silhouette Score: {silhouette:.2f}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index:.2f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_index:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "print(f\"Adjusted Mutual Information (AMI): {ami:.2f}\")\n",
    "\n",
    "# Q: What do these numbers mean? What are the scores measuring?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783c56d-e61e-426f-9ef5-76a2125ac056",
   "metadata": {
    "id": "f783c56d-e61e-426f-9ef5-76a2125ac056"
   },
   "source": [
    "This provides a mild increase in the clustering skill, which is nice to see.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Explore what happens if I don't standardise. Why and where do you think the problem is? (The former you should know, the latter you might need to think a litte bit.)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> I didn't do `train_test_split`, but consider what happens if I do that, and see how good the label predictions are.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Do an ensemble of these and see how robust the scores are.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Explore above but with different hyperparameters, and evaluate the sensitivity of scores to hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120ad12-d852-47b9-8c50-040a8765b524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "166e2b3d-c401-49d1-99dc-e7fa1e15ecab",
   "metadata": {
    "id": "166e2b3d-c401-49d1-99dc-e7fa1e15ecab"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Other clustering algorithms\n",
    "\n",
    "There are a whole load of them at the [clustering](https://scikit-learn.org/stable/modules/clustering.html) page of `sklearn`. Have a look at them and see what you make of them, particularly of Gaussian Mixture Models (GMM), which I reference in the lecture but don't actually use it.\n",
    "\n",
    "(You can also explore the other [manifold learning](https://scikit-learn.org/stable/modules/manifold.html) methods.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eedd5-d90e-494f-af11-58189c36fc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74d42cac-627c-433c-9781-9b42212c9214",
   "metadata": {
    "id": "74d42cac-627c-433c-9781-9b42212c9214"
   },
   "source": [
    "## 2) Primer for the assignment: using labels to improve models for `penguins`\n",
    "\n",
    "Use the identified clusters to train different linear models, or add the cluster labels in as an extra feature to train linear models. This is something you should do for the assignment with Argo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7eb4e-cd1e-494b-8577-8d63039d3719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d47867-41c7-4c4d-aa3e-fa097fd3f641",
   "metadata": {
    "id": "e2d47867-41c7-4c4d-aa3e-fa097fd3f641"
   },
   "source": [
    "## 3) Clustering and/or manifold learning of cats (possibly also data cleaning)\n",
    "\n",
    "(May be useful for dealing with the penguins/turtle dataset and in classification tasks.)\n",
    "\n",
    "Consider applying clustering and/or manifold learning to cat images: this is a bit like what is done for the [digits dataset](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html), but larger dimension. I would start with the smaller `cats.csv` dataset first to see if you can identify clusters of data.\n",
    "\n",
    "If you are feeling brave and/or have some computational resources lying around, consider using the larger `cat_bw_enlarged.csv` dataset, and possibly use these approaches to detect ***outliers*** (there are definitely outliers or \"not good\" images in the dataset). This may be one way to use machine learning to help clean data, then feed the cleaner dataset back in for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a8152-678d-4886-9174-c64072b8499c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
