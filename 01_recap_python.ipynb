{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4dc7f5b-8740-4ce8-89f4-48a523f77203",
   "metadata": {},
   "source": [
    "*updated 23 Jul 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 4303 \"AI and Machine Learning in Marine Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES4303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaf749-1480-4de3-8fda-cd7b19e11746",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 01: Python recap and data handling\n",
    "\n",
    "The course here will use [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) through [Jupyter notebooks](https://jupyter.org/). I chose Python because:\n",
    "* It's free (i.e. not [MATLAB](https://en.wikipedia.org/wiki/MATLAB) or SPSS)\n",
    "* It's not [R](https://en.wikipedia.org/wiki/R_(programming_language)) (I hate R syntax personally, but it is very a powerful tool)\n",
    "* Python is used widely, has a lot of packages built in, pretty mature with userbase and support (an appropriate Google search will most of the time help debugging)\n",
    "* Familiarity to me\n",
    "\n",
    "I will openly admit I do not write Python in a Pythonic way: I started on MATLAB until MATLAB screwed up vector graphics outputs for me, so I rage quit and went to Python. The code provided here is certainly not the cleanest way to do it (this is sometimes by design), nor is it the most idiomatic way of doing it, but it should (mostly?) work and do the intended thing.\n",
    "\n",
    "## <span style=\"color:red\">!!! NOTE !!!</span> \n",
    "\n",
    "The content here has OCES 3301 as a pre-requisite, and largely assumes familiarity with Python and some of the associated packages (see the list of packages to be loaded). The course content is available at https://github.com/julianmak/OCES3301_data_analysis for reference purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47164f-e5c5-4efb-8c17-77ea900d61cc",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Recapping Python through various data handling\n",
    "\n",
    "What it says on the title. Just going to load and do basic manipulations of data previously considered in OCES 3301, as well as some new ones that will be used for demonstration purposes. \n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. The present notebook is to check you can in fact load (almost) all the data that will be used for the course. If you are having trouble now then it really should to be fixed (e.g. ask for help), because there will be issues for the remaining content...\n",
    "> 2. Demonstrates some Python loading/plotting approaches.\n",
    "> 3. Demonstrates some manipulations of array data, as well as `pandas` and `xarray` dataframes.\n",
    "\n",
    "Going to load a bunch of relevant libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253bf34-1986-453d-966a-d93a7bacadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25c1ee-46ab-4772-b1b3-ac3ca6a9ca90",
   "metadata": {},
   "source": [
    "---\n",
    "## a) Numerical data: python generated\n",
    "\n",
    "Name of the game for most of this course is to turn the data we read into numbers, manipulate those into a form that the Python data science + Machine Learning packages understands, and then feed them in. So it's probably useful to start with those.\n",
    "\n",
    "Below is a simple example of the function\n",
    "\\begin{equation*}\n",
    "    f = \\sin(t)\n",
    "\\end{equation*}\n",
    "modified in various ways (this is the same example used in `07_time_series` of OCES 3301). I am going to generate an ***array*** of numbers for $t$, and this is then fed into a function that spits out another array.\n",
    "\n",
    "> Note: For displaying to screen I will mostly use ***fstrings*** (e.g. `f\"STUFF\"` with the preface `f` before the string marks `\" \"`), although occasionally I will use `r` instead if I need some specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb8576-053c-492b-92f1-41733a372a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vec   = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f       =   np.sin(t_vec)\n",
    "f_pos   = 2*np.sin(t_vec)\n",
    "f_neg   =  -np.sin(t_vec)\n",
    "f_shift =   np.sin(t_vec - np.pi / 2.0)\n",
    "\n",
    "print(f\"t_vec = {t_vec}\")\n",
    "print(\" \")\n",
    "print(f\"f = {f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66fe68-bf80-4241-a42f-5a6ec52f1dd5",
   "metadata": {},
   "source": [
    "The above as shown is not hugely useful since it's just a dump of numbers. We can visualise this accordingly. Left plot shows it as a function of time (e.g. $[f_0, f_1, \\ldots]$ against $[t_0, t_1, \\ldots]$), right plots shows it as one function plotted against another (e.g. $[f_0, f_1, \\ldots]$ against $[g_0, g_1, \\ldots]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ca625-03cb-49c4-b021-6ad4c791876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# award winning graph\n",
    "\n",
    "# 2x4 grid, firsrt graph takes up a 2x2 space located first at the upper left corner (0, 0)\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.subplot2grid((2, 4), (0, 0), colspan=2, rowspan=2)\n",
    "ax.plot(t_vec, f    ,   \"C0\", label=r\"$f$\")\n",
    "ax.plot(t_vec, f_pos,   \"C1\", label=r\"$f^+$\") \n",
    "ax.plot(t_vec, f_neg,   \"C2\", label=r\"$f^-$\")\n",
    "ax.plot(t_vec, f_shift, \"C3\", label=r\"$f_{\\rm shift}$\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# subsequent graphs are 1x1 but with a change in the location\n",
    "ax = plt.subplot2grid((2, 4), (0, 2))\n",
    "ax.scatter(f, f, color=\"C0\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 4), (0, 3))\n",
    "ax.scatter(f, f_pos, color=\"C1\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f^+$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 4), (1, 2))\n",
    "ax.scatter(f, f_neg, color=\"C2\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f^-$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 4), (1, 3))\n",
    "ax.scatter(f, f_shift, color=\"C3\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f_{\\rm shift}$\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=1.0) # give the graph a bit of padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301b39b-1f75-4164-8e3d-aa62cc216a81",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> This was previously used to demonstrate lag (linear) correlations. Convince yourself that the correlations of the right hand side subplots are (going clockwise) 1, 1, 0 and -1 as they should be, corresponding accordingly to what you would suspect from looking at the entries in the left hand side subplot. Convince yourself that the **lagged correlation** looks like a sine (or cosine) curve.\n",
    "\n",
    "Can do stuff for multi-dimension data, but going to do this together with reading data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf30356-8b6d-4dd1-a569-5bb446ed5556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c0a5d1-ea25-4e09-ad70-5014b9476e01",
   "metadata": {},
   "source": [
    "---\n",
    "## b) Reading numerical data from file\n",
    "\n",
    "<img src=\"https://i.imgur.com/rKcpZzr.jpg\" width=\"400\" alt='cursed panda'>\n",
    "\n",
    "I am going to rely on `pandas` or `xarray` to read the data provided from file (or remotely via an internet connection if the files are small enough), and then do some manipulations and/or plotting with these; these can be done in principle via other means (see OCES 3301 for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26f688-0fee-411f-9138-514416c240b1",
   "metadata": {},
   "source": [
    "## El Nino 3.4 data\n",
    "\n",
    "This is a text file shown in the lecture slides and is just a text file. Going to call this remotely and spit our the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668e9a5-e849-4498-9c39-e2ea4dfab5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"elnino34_sst.data\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/elnino34_sst.data\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0db1c-b308-44ca-b7fc-7ea99b8ce0ee",
   "metadata": {},
   "source": [
    "Generally pandas ***tries*** to read things assuming sensible layout etc., but that can fail if the data is not cleaned up (and most data is uncleaned for your exact purpose). Here we have headers and misc. things we don't really need, leading to things being read into a single block. \n",
    "\n",
    "It is generally a good idea to have a look at the raw data file first to see what it consists and anticipate what things you might need to do. In this case, optional arguments needs to be provided (e.g. delimiter, separator, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388efe1-2879-44f6-8c2b-13ef1012ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can give it a few more details to make it easier for pandas to help us\n",
    "df = pd.read_csv(path,\n",
    "          sep='\\s+',     # this used to be delim_whitespace=True,\n",
    "          names=[\"year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"],\n",
    "          skipfooter=4,  # chop out some lines\n",
    "          skiprows=1,    # chop out some unnecessary lines\n",
    "          false_values=-99.99,\n",
    "          engine=\"python\")\n",
    "df = df.replace(-99.99, np.nan) # replace missing values with NaNs (not a number)\n",
    "df = df.set_index(\"year\")       # sets the index to be the year column\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be92af-4dee-4a14-b153-42d5b23a0aac",
   "metadata": {},
   "source": [
    "Notice there are `NaN`s in the data, which itself is not an issue. The pandas dataframe `df` has `72 rows x 12 columns`, which is 72 years of data every month over 12 months. We can do a quick dirty plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eda44e-7cb1-4279-85d1-ac16015d4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not an award winning graph\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "df.plot(ylabel=r\"${}^\\circ\\mathrm{C}$\", ax=ax)  # pass some keywords in\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcddd5b6-96e1-4b24-ad17-370b93f2c7dc",
   "metadata": {},
   "source": [
    "This case it's treating `months` (as the header) as a category, i.e. plotting all the January temperatures as a function of `year`. This isn't necessary what we want: we probably want it as a time-series with increasing time.\n",
    "\n",
    "There are ways to do the reshaping in pandas, but for demonstration I am going to do this in native `numpy`. I am going to:\n",
    "\n",
    "1. load the pandas `df` frame into a numpy array with `df.values`.\n",
    "2. I want to keep the column ordering but remove the rows (e.g. have 1st row of 12, then 2nd row of 12 etc.), to be done via `.reshape(SIZE)`\n",
    "3. above can be done with `.flatten()` also\n",
    "4. I am additionally going to compute the linear ***line of best fit*** (cf. `07_time_series` in OCES 3301), which I will need if I want to detrend the data to get the ***anomalies*** (although I don't use it here)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> by default `.reshape()` and `.flatten()` both use `order=C` as a default option, which gives the right thing in this case. Try this with `order=F` and convince yourself that is the wrong thing to do (the positions of `NaN`s would help)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb5413-d327-40f6-8b33-9a0e08f63ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "print(f\"data has shape {data.shape}\")\n",
    "data = data.reshape(data.size)  # data.size gives the total the number of entries\n",
    "print(f\"data now has shape {data.shape} after reshape or flatten\")\n",
    "print(\" \")\n",
    "\n",
    "# line of best fit via polyval (only because I don't want to load scipy)\n",
    "# need to remove NaNs first\n",
    "data_dum = data[~np.isnan(data)]  # find the NOT NaNs\n",
    "p = np.polyfit(np.arange(len(data_dum)), data_dum, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(data, label=\"data\")\n",
    "ax.plot(np.polyval(p, np.arange(len(data_dum))), label=\"LOBF\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_ylabel(r\"${}^\\circ\\mathrm{C}$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72512fd6-6208-48d3-9e4f-a385aef674a9",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> I was lazy and didn't provide the time array. Create a time array and do a proper regression (be careful of units). Then you can get a magnitude of a global warming trend from `p[0]` (which is the gradient of the straight line).\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Do the above but with `scipy` or `sklearn`.\n",
    "\n",
    "See extended exercises for more things to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b82d05-206d-4cf5-bd43-271be4167eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c55571e-e962-4400-a485-a9b57cfebd58",
   "metadata": {},
   "source": [
    "## Penguin data\n",
    "\n",
    "<img src=\"https://www.boredpanda.com/blog/wp-content/uploads/2020/08/cats-standing-like-penguins-fb-png__700.jpg\" width=\"500\" alt='cursed penguins'>\n",
    "\n",
    "The [Palmer Penguins](https://cran.r-project.org/web/packages/palmerpenguins/readme/README.html) data was compiled as a replacement/alternative to the standard [iris data](https://en.wikipedia.org/wiki/Iris_flower_data_set) because of racism/eugenics reasons of Ronald Fisher (look it up if you are interested). A mildly touched up version is given here as `penguins.csv` (or https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/penguins.csv; I removed some columns and some `NaN`s). We are going to be using this dataset quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b448c0c-e88c-42a7-8ac9-d2320e75b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e623544-2ad9-4948-bc93-9ae3b7af3203",
   "metadata": {},
   "source": [
    "So this one is a text file but notice the headers are also loaded and are in fact useful because it tells you the data entries and also the units (which is more than can be said for a lot of data...) Notice also the species column is text while others are numbers; we will end up converting the species entries to numerical values in due course.\n",
    "\n",
    "Zeroth step of data analysis/exploration is to actually plot out the data first. Here are some random things I thought that could be done to demonstrate plotting/visualising and calling of things from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da672049-34a8-4f77-90d0-046a5e41d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of one of the randomly chosen variables cycling the species\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "target_var = np.random.choice(target_vars)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.hist(df[df[\"species\"] == species][target_var], label=species, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(f\"{target_var}\")\n",
    "ax.set_ylabel(\"frequency\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30382b4d-324b-4841-91b5-ed84d7ae38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 3d plot of three random choices of variables\n",
    "\n",
    "from mpl_toolkits import mplot3d  # load a package for 3d plots\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "target_var = np.random.choice(target_vars, 3, replace=False)  # 3 unique choices\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_var[0]], \n",
    "               df[df[\"species\"] == species][target_var[1]],\n",
    "               df[df[\"species\"] == species][target_var[2]],\n",
    "               label=species\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_var[0]}\")\n",
    "ax.set_ylabel(f\"{target_var[1]}\")\n",
    "ax.set_zlabel(f\"{target_var[2]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793d451-7a00-4a79-99cd-5f8301b0961f",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> In the histogram plot I was deliberately lazy and didn't provide pre-defined bin edges (so `ax.hist` ended up choosing it). Pre-define the bin edges and do the binning of the data so that data from every species uses the same bins. You may want to use `np.histogram` instead, and then throw the outputs from there into `ax.hist` accordingly.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> The histogram plot shows frequency for now, but turn that into a probability.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Explore other combinations of scatter plots in both 2d and 3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db4efc-7fc9-4ad7-a513-58a9e18b4ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ccd5f6-edfb-4ea5-b131-dd4f2ec8d373",
   "metadata": {},
   "source": [
    "## Gridded data\n",
    "\n",
    "By gridded I mean these are data that have pre-defined co-ordinates that sits on a grid, such as (longitude, latitude) or similar. An example of this is satellite data: initially the data is per swarth, but given enough swarths some filling in can be done and the data put on a regular grid that is more useful for end users. One ongoing application of ML in oceanography would be instead of waiting for enough swarths, maybe you could use ML to fill in the gaps instead.\n",
    "\n",
    "The one I am showing here is from a simulation (sample from [NEMO ORCA0083-N01](https://gws-access.jasmin.ac.uk/public/nemo/)). The original dataset is REALLY big, so I downsized it quite significantly. The data is in the [NetCDF](https://en.wikipedia.org/wiki/NetCDF) format which is supposed to be self-describing (the one I made is not quite that). Going to open this with `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac697a-c86d-4e59-bdc7-13063d220991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# would do \"local\" for this one, because the filesize is not small (~45 MB)\n",
    "\n",
    "# could do this once and for all (as long as you save the file) with\n",
    "# !wget https://github.com/julianmak/OCES4303_ML_ocean/raw/refs/heads/main/current_speed.nc\n",
    "\n",
    "import fsspec  # for caching the file if using remote option\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"current_speed.nc\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES4303_ML_ocean/raw/refs/heads/main/current_speed.nc\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = xr.open_dataset(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7fcd3-dde1-4bc1-b41d-b5282cd2fe21",
   "metadata": {},
   "source": [
    "This one is `(time, lat, lon)`, and although I didn't write the units in it is `m s-1`. Here we can select one time and plot out the data as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db7fbc-92ad-4e94-a37f-ad7a77617c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using contourf here, could do pcolor also\n",
    "t_ind = 0\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.axes()\n",
    "cs = ax.contourf(df[\"lon\"], df[\"lat\"], df[\"speed\"][t_ind, :, :], 31)\n",
    "ax.set_xlabel(r\"lon $(^\\circ)$\")\n",
    "ax.set_ylabel(r\"lat $(^\\circ)$\")\n",
    "ax.set_title(df[\"time\"][0].values)  # load as string to remove other xarray descriptors\n",
    "cax = plt.colorbar(cs)\n",
    "cax.ax.set_title(r\"$\\mathrm{m}\\ \\mathrm{s}^{-1}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb6a8b-cb6e-4420-8568-24c97d4a856b",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Plot out longitudinal or meridional slices instead.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Select one location and plot out the time series.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Make a movie out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309631a-095e-4a95-9569-df85903ae800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29aa638-7d17-4c22-b992-434ce0981f80",
   "metadata": {},
   "source": [
    "## Argo data\n",
    "\n",
    "[Argo](https://argo.ucsd.edu/data/) is a system of autonomous floats that are put into the ocean, floating around with the currents, and periodically does vertical sections to take in-situ measurements of things like temperature, salinity, pressure, and so forth down to about 2000 m depth; see below for the schematic. There are increasing interest in [BGC-Argo](https://biogeochemical-argo.org/) that measure quantities relevant to biogeochemistry, and [deep Argo](https://argo.ucsd.edu/expansion/deep-argo-mission/) that go down to 4000 m. See OCES 3301 for more description.\n",
    "\n",
    "<img src=\"https://argo.ucsd.edu/wp-content/uploads/sites/361/2020/06/float_cycle_1-768x424.png\" width=\"600\" alt='Argo'>\n",
    "\n",
    "> NOTE: The namesake of argo is related to the [JASON](https://en.wikipedia.org/wiki/Jason-1) satellites if you know your Greek mythology.\n",
    "\n",
    "The float data here are vertical sections at specific locations of space, and can be regarded as data that is more \"raw\" than the gridded data. The data provided here is in the `zarr` formatt which can be opened with `xarray` also. \n",
    "\n",
    "Some care needs to be taken to obtain a copy of this. Would highly recommend not loading this remotely, because the content is quite big.\n",
    "\n",
    "> <span style=\"color:red\">!!! NOTE !!!</span> (JM 15 Apr 2025): If you are on Colab, you probably need to mount and do a separate upload of the data.\n",
    "> 1) Go to https://drive.google.com/drive/folders/1JJ0cpshu6-JE8wp93UsHuqy6V33rQy7s?usp=sharing\n",
    "> 2) Download the folder\n",
    "> 3) Upload that to your own instance of Colab\n",
    "> 4) Mount with `from google.colab import drive; drive.mount('/content/drive')` and then proceed as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff19374-bc25-40f9-b134-345d7e8d5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is slightly out-of-date and will fail with \"consolidated\" option but will load\n",
    "# silencing the warning\n",
    "df = xr.open_zarr(\"./GLOB_HOMOGENEOUS_variables.zarr/\", consolidated=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca22157-a8a7-4189-be96-a711ce0072e5",
   "metadata": {},
   "source": [
    "So note that the data is arranged as `(N_PROF, DEPTH)`, and `TIME`, `LATITUDE` and `LONGITUDE` are the variables tagged to `N_PROF`. Sample plot looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d67a7b-8943-4679-b833-399fa953f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out what the observation data actually looks like in geographical space\n",
    "\n",
    "nl = 20 # change this index to plot different depths (as an index entry)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "\n",
    "# temperature\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cs = ax.scatter(df.LONGITUDE, df.LATITUDE, 10, df.TEMP[:,nl], \n",
    "                cmap=plt.get_cmap('Spectral_r'), zorder=3)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.set_title(f\"Temp at {df.DEPTH[nl].values} m\")\n",
    "\n",
    "# salinity\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cs = ax.scatter(df.LONGITUDE, df.LATITUDE, 2, df.PSAL[:,nl], \n",
    "                alpha=.5, cmap=plt.get_cmap('viridis', 5), zorder=3)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.set_title(f\"Salinity at {df.DEPTH[nl].values} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c76ca2-4d63-4eaf-a9a4-3db628123365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out TS-diagrams at different depths\n",
    "\n",
    "nl = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(df.PSAL[:, nl], df.TEMP[:, nl], \"o\", markersize=2, label=\"total\")\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f\"TS diagram at {df.DEPTH[nl].values} m\")\n",
    "\n",
    "nl = 20\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(df.PSAL[:, nl], df.TEMP[:, nl]  , \"o\", markersize=2, label=\"total\")\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f\"TS diagram at {df.DEPTH[nl].values} m\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598e5e5-cb8c-446d-b3dc-eb15da75ebea",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> So from the TS diagram you notice that are some outliers that probably should be removed (e.g. water that is too fresh is unlikely under typical oceanic conditions). Come up with criteria to drop these points from `df`, and do the plots again. This is an important part of data pre-processing before throwing it into the ML algorithmcs, following the ***Garbage In Garbage Out*** principle.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Subset these by geographical locations (e.g. Atlantic Ocean using whatever defensible criterion you like), and either plot these separately, or plot them together with the labels.\n",
    ">\n",
    "\n",
    "We will come back to this dataset later (e.g. `02` for data cleaning and scaling, `04` for clustering and later ones for doing interpolation and/or gap filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3bc56-8459-45a2-b70c-e5ae0eb7fdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd4de47-d652-4091-975a-dca80ad3cb50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## c) Images as numerical data\n",
    "\n",
    "Just like we can visualise data as an image, we can sometimes go the other way and get data out of an image:\n",
    "\n",
    "1) One possible example might be that you have chlorophyll concentration measurements, which gives some shades of green in the image. Then a useful thing might be the reverse: you can consider the case of measuring greeness from a satellite to infer for the chlorophyll concentration.\n",
    "2) I want to automatically classify species of fish or penguins or whatever from a long video segment.\n",
    "3) I have broken images that I may want to fill out.\n",
    "\n",
    "I am going to use some `jpg` files I have handy to demonstrate images as arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234b0e2-aab8-479c-b597-807b913caf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"broccollie.jpeg\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES4303_ML_ocean/raw/refs/heads/main/broccollie.jpeg\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "data = plt.imread(file)\n",
    "ax = plt.axes()\n",
    "ax.imshow(data)\n",
    "ax.set_title(f\"a broccolie of shape {data.shape}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bded57d-f69d-430a-ab2b-6f3bc1241691",
   "metadata": {},
   "source": [
    "As can seen from querying the loaded array, the array is of size `(pixels, pixels, RGB)` where `RGB` is the strength of (red, green blue), and this goes from 0 to 255 for reasons you can look up if you want.\n",
    "\n",
    "It's just an array so all the usual things can be done to it. The example below converts the RGB image to grayscale using the formula\n",
    "\\begin{equation*}\n",
    "    \\mathrm{gray} = 0.299 \\mathrm{Red} + 0.587 \\mathrm{Green} + 0.114 \\mathrm{Blue}.\n",
    "\\end{equation*}\n",
    "The formula assumes the RGB values lie between 0 and 1 so some conversion is needed, but that's easy (just divide by 255).\n",
    "\n",
    "> NOTE: For plotting you could of course just plot is as grayscale, but this is for demonstrating how to manipulate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b66f2-090c-40b2-9a48-67de161c0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise data then convert\n",
    "data_bw = data / 255\n",
    "data_bw = 0.288 * data_bw[:, :, 0] + 0.587 * data_bw[:, :, 1] + 0.114 * data_bw[:, :, 2]\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.imshow(data_bw, cmap=\"gray\")\n",
    "ax.set_title(f\"a broccollie of shape {data_bw.shape}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60cacd-eac1-4625-8549-1060c72c90d8",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Consider passing these through blurring or sharpening fitlers. `scipy` has a few, or you can do it from scratch by specifying the convolution or deconvolution kernels accordingly (cf. `08_times_series` and `10_fun_with_maps` in OCES 3301).\n",
    "\n",
    "Below case is a stack of images that are written into a `csv` file, where one dimension denotes all the pixels, and the other dimension denotes the sample number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a1e3f-2fd1-44b0-a9c9-dd91162c628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/cat.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "# going to transpose this so the shape is (image number, pixels)\n",
    "df = pd.read_csv(path, header=None).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6e674-5433-45f7-b897-bbe306fc35c5",
   "metadata": {},
   "source": [
    "It turns out this is 80 images of cats, of 64 by 64 pixels each ($64^2 = 4096$). This one requires unflattening/reshaping the arrays for the images to make sense. For ML applications with actually feed in flattened data into algorithms; see later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c797c3-0ada-4660-9e0c-8bbeada08ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, reshape data and then plot one of the guys out\n",
    "\n",
    "cats = df.values\n",
    "ind = np.random.randint(cats.shape[0])\n",
    "\n",
    "# transpose back for image display purposes\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax = plt.axes()\n",
    "ax.imshow(np.reshape(cats[ind, :], (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_title(f\"cat {ind+1} / {cats.shape[0]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634ae3-448d-42d8-a398-a6e4d25f6a60",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> As a python exercise, try and use a loop to plot five of these but randomly choosing the image number to plot. Make sure the randomly selected indices are distinct.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> You can try reshaping it in different ways (e.g. instead of `.reshape(64, 64)` try other numbers that multiple to `4096`), and convince yourself the choice taken here is the only sensible one.\n",
    ">\n",
    "Point here is that if you can deal with these sample images you can in principle deal with other images (e.g. fish/coral/snails/satellite/remote sensing images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acfdc4-5aaa-45c0-a178-b9ca99007c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3273f3e7-88b4-4b78-a898-fb8f5103c9f9",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Delay embedding (Takens' embedding)\n",
    "\n",
    "Taking the sine curve example, consider doing the plots like the right hand side, but instead do e.g.\n",
    "\\begin{equation*}\n",
    "    [f_1, f_2, f_3, f_4, \\ldots, f_N]\n",
    "\\end{equation*}\n",
    "against\n",
    "\\begin{equation*}\n",
    "    [f_0, f_1, f_2, f_3, \\ldots, f_{N-1}],\n",
    "\\end{equation*}\n",
    "i.e. shift the array by one index (or more if you want). You will need to be careful about array sizes and calling array entries that are less than zero (this will lead to wrap around, e.g. `f_{-1} = f_{N}` and `f_{-2} = f_{N-1}`; the sine curve example is periodic so it doesn't matter, but it might do for more general cases). \n",
    "\n",
    "You may want to consider writing this a subroutine that takes in an array and spits out two arrays, one with a shifted index, and both having the correct size.\n",
    "\n",
    "What you should get are ellipses with different eccentricities depending on the time-lag. Convince yourself that makes sense.\n",
    "\n",
    "The above is related to [Takens' theorem](https://en.wikipedia.org/wiki/Takens%27s_theorem) and we may or may not come back to this in the bonus lectures, e.g. [Empirical Dynamic Modelling (EDM)](https://en.wikipedia.org/wiki/Empirical_dynamic_modeling), [Topological Data Analysis (TDA)](https://en.wikipedia.org/wiki/Topological_data_analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfdd77-2557-4ea6-bf7b-90e2aeba6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline arrays for doing lag embedding with\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, 61)\n",
    "f     = np.sin(t_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bb4f5-a5e5-466b-956c-61c0a54d840a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5dd5675-7364-4421-979f-d366d095141c",
   "metadata": {},
   "source": [
    "## 2) El Nino data manipulation\n",
    "\n",
    "Starting from the El Nino 3.4 time series data, probably detrend it to get the anomalies. Provide a threshold criteria to classify El Nino and La Nina events using an analogous criterion to e.g. https://ggweather.com/enso/oni.htm. You may or may not want to compute some smoothing / running averages.\n",
    "\n",
    "Could also compute the power spectrum or similar to obtain the magnitude of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8f883-7798-49f1-937e-f3617c88ca72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dbc855b-12a7-4d21-ad61-70a56404f2a7",
   "metadata": {},
   "source": [
    "## 3) Turtle + penguin data\n",
    "\n",
    "#### (Probably relevant to one of the assignments.) \n",
    "\n",
    "Have a look and see what is in this dataset: https://www.kaggle.com/datasets/abbymorgan/penguins-vs-turtles. This may be used for one of the marked assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ffb70-ccd9-477e-9de1-a23235503195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c83aeaa-0a79-40f0-86c4-c5c0d785bf6d",
   "metadata": {},
   "source": [
    "## 4) Satellite data\n",
    "\n",
    "#### (May be relevant to one of the assignments.) \n",
    "\n",
    "Obtain some satellite grid and/or track data. Would recommend having a look at the [Sentinel Data Space](https://dataspace.copernicus.eu/data-collections/copernicus-sentinel-data); we may also use this in one of the worked examples and/or marked assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1fe5a-47bd-4adf-8b84-24ad915bcd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22199428-c613-4a13-97e3-95f1f2f0c6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
