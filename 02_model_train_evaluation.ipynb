{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ef4352-73fd-45f9-8bb7-f656c45b94d6",
   "metadata": {},
   "source": [
    "*updated 27 Jul 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 4303 \"AI and Machine Learning in Marine Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES4303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad87ebc-f634-45a3-92a5-f071f63a664c",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Data splitting, scaling, model skill, and validations\n",
    "\n",
    "This is one of the few cases where the material here is not completely self-contained, because otherwise this notebook becomes quite long and sidetracking quite a bit. You should refer to the lecture slides for the theoretical background.\n",
    "\n",
    "The main moral of the story is that ***what you don't do in statistics you should not do in data-driven / ML methods***; see OCES 3301. Examples include:\n",
    "1. Do lots of statistical trials and only report the one that gave you the result you want\n",
    "2. Torturing the data until it confesses\n",
    "3. You should cross validate and provide robustness checks\n",
    "4. Design on how you obtain the data and how you clean the data is actually important\n",
    "5. etc...\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/extrapolating.png\" width=\"450\" alt='extrapolating'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103657e1-2934-4663-9c69-90b5d0c7bdae",
   "metadata": {},
   "source": [
    "Here we are going to demonstrate some aspects of what it says on the title, using linear regression and some of the `scikit-learn` interface.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Demonstrates utility and syntax of the `scikit-learn` package\n",
    "> 2. Highlight some inherent randomness in data-driven methods\n",
    "> 3. Given the above, highlight the need to evaluate robustness of model skill (which requires defining what is meant by \"skill\")\n",
    "\n",
    "Going to load a whole load of basic things; specific parts of `scikit-learn` will be loaded when the topic comes up. I am going to demonstrate most of my points using the `penguin` data, so I also need `pandas` but just at the beginning (mostly copy and pasting some code from the last workshop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5194e0e-0436-4a9d-ad22-b445e47dbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9d4de-20a8-4a04-8577-003ba07c4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ff128-2d7d-46f8-9e03-288b201ab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 2d plot of two specific variables in the penguins data\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "ind = [3, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "for species in np.sort(df[\"species\"].unique()):   # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_vars[ind[0]]], \n",
    "               df[df[\"species\"] == species][target_vars[ind[1]]],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09790ffc-5785-46e5-97b4-d790bff14c6d",
   "metadata": {},
   "source": [
    "---\n",
    "## a) Dataset splitting\n",
    "\n",
    "The usual approach is to split the data into:\n",
    "\n",
    "* ***training data***, which is passed in during model training\n",
    "* ***testing data***, which is used to evalute the ***skill*** of a model (whatever you want skill to mean)\n",
    "* ***validation data***, which is for tuning internal ***hyperparameters***, and is exposed to the model during training\n",
    "\n",
    "Mostly going to deal with the first two. Two \"obvious\" (!?) questions to ask for the dataset splitting would mainly concern whether you want a specific selection, and how much data you want in each dataset. There is some arbitrariness in this and answer might be context dependent...\n",
    "\n",
    "Without knowing what the data looks like, probably the most sensible thing to do is to just select data randomly (i.e. sampling from a uniform distribution). Doing a 80:20 training testing dataset split also seems reasonable. You can do this by hand, or use the `train_test_split` in `sklearn` to do this also. Below demostrates the syntax of that and provides some illustration of what is going on.\n",
    "\n",
    "> NOTE: Below I've actually loaded the arrays into memory, but can actually pass in `pandas` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca11c5-79a7-4d0f-bc5c-fc62ea8d2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split syntax demonstration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# select data based on choice of \"ind\" above:\n",
    "X = df[target_vars[ind[0]]].values  # actually load the data into memory\n",
    "\n",
    "# going to subset data twice to demonstrate a point\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    X_train, X_test = train_test_split(X, test_size=0.2) # 20% of data in test\n",
    "    ax.plot(X_train, f'C{i}o', alpha=0.7, label=f\"split {i}\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63223c16-841e-4e2b-ad19-2fe9012ed9cc",
   "metadata": {},
   "source": [
    "It's not that clear above, but the splitting is random and the data selected is not completely overlapping. It's probably clearer in the case where I select two variables and do this as a scatter plot. The below code demonstrates also how you can pass in multiple arrays to be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9026b2d-211f-4ac5-a96f-42c882405f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data based on choice of \"ind\" above:\n",
    "X = df[target_vars[ind[0]]].values  # actually load the data into memory\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "\n",
    "# going to subset data twice to demonstrate a point\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(2):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "    ax.plot(X_train, Y_train, f'C0o', alpha=0.7, label=\"training data\")\n",
    "    ax.plot(X_test, Y_test, f'C1^', label=\"testing data\")\n",
    "    ax.set_xlabel(f\"{target_vars[ind[1]]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(f\"{target_vars[ind[0]]}\")\n",
    "    ax.grid(lw=0.5, zorder=0)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe67f8-de0b-4693-a16e-85537552aea7",
   "metadata": {},
   "source": [
    "For reproducibity (e.g. in an assignment say), then you can specify `random_state=SEED` in the `train_test_split` to force it to split in a specific way (basically specifying the random seed). The below basically shows two identical images.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Check the split arrays are in fact exactly identical in the contents and the order when the random seed is specified (there should be no permutations in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb722541-fd45-4b27-9b3c-582482b1316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to subset data twice to demonstrate a point\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(2):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "    ax.plot(X_train, Y_train, f'C0o', alpha=0.7, label=\"training data\")\n",
    "    ax.plot(X_test, Y_test, f'C1^', label=\"testing data\")\n",
    "    ax.set_xlabel(f\"{target_vars[ind[1]]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(f\"{target_vars[ind[0]]}\")\n",
    "    ax.grid(lw=0.5, zorder=0)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92856f-6d77-48ec-b7d9-612eb0c9848c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d37b91-d307-47bb-a53a-4d940e5a22fa",
   "metadata": {},
   "source": [
    "---\n",
    "## b) Data scaling/transformations\n",
    "\n",
    "If you have't changed my default settings then one thing you can argue is that the units of the selected data are not even the same (one is a length and the other is a weight), so how can we even sensibly/meaningfully compare them? Further, given a ML model depends on data input, would this discrepancy not cause problems for the model? \n",
    "\n",
    "A quick answer is that \"you can't/shouldn't\" and \"yes it will do, although the question (and thus the related answer) is probalby much more subtle... Without going into too much detail, it really is also context dependent, and a standard (!?) thing to do is to normalise and scale the data distributions accordingly.\n",
    "\n",
    "> NOTE: \"Standard\" does not mean it is the \"best\" all the time.\n",
    ">\n",
    "> I like to try and think of problems from a geometry point of view, and the way I'd phrase the above problem is effectly asking what is the ***metric*** of interest. From a machine learnig point of view, see [information geometry](https://en.wikipedia.org/wiki/Information_geometry) and the textbook by the late [David MacKay](https://www.inference.org.uk/mackay/itila/book.html) for an overview of this maybe.\n",
    "\n",
    "One way to do this is to ***assume*** (the \"assume\" part is important) data follows a Gaussian/normal distribution, i.e. for data (random variables) $X$ and $Y$, we have $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X)$ and $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y)$. While each of the data sets would have their own mean and standard deviations $\\mu$ and $\\sigma$, we can remove the individual the mean and scale by the standard deviation so the transformed data follow a standardised Gaussian distribution, i.e. $\\tilde{X}, \\tilde{Y}\\sim \\mathcal{N}(0, 1)$. Then we can compare them (maybe). The `StandardScaler()` below basically does this. I am going throw the data in individually first to demonstrate some basic syntax.\n",
    "\n",
    "> NOTE: `sklearn` expects data arrays to have the shape `(n_samples, n_features)`. The 1d arrays I throw in will fail although the warning will ask for a `.reshape`; the one we want is `.reshape(-1, 1)` in this case (it basically just adds an extra dimension to it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae28b0d-1d0e-410f-9fb0-5b93e4068a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of standard scaler: comparisons of PDFs (i.e. histograms)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[target_vars[ind[0]]].values  # actually load the data into memory\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "\n",
    "# sklearn syntax: expects (n_samples, n_features), reshape below is needed\n",
    "print(f\"shape before = {X.shape}\")\n",
    "X = X.reshape(-1, 1)\n",
    "print(f\"shape after  = {X.shape}\")\n",
    "print(\" \")\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "# initialise scaler, then fit, and then transform\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(X)   # data is n samples 1 feature\n",
    "X_scale = scaler_X.transform(X)\n",
    "scaler_Y = StandardScaler()\n",
    "scaler_Y.fit(Y)\n",
    "Y_scale = scaler_Y.transform(Y)\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# raw unscaled histograms\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.hist(X, density=True, color=\"C0\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.hist(Y, density=True, color=\"C1\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$Y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.hist(X_scale, density=True, color=\"C0\", alpha=0.7, label=r\"$\\tilde{X}$\")\n",
    "ax.hist(Y_scale, density=True, color=\"C1\", alpha=0.7, label=r\"$\\tilde{Y}$\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(r\"$\\tilde{X}$ and $\\tilde{Y}$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4139d1ea-3d0c-457e-9c0a-d2273e3a9f50",
   "metadata": {},
   "source": [
    "Hopefully you are convinced that it would be ridiculous to try and plot the left and middle one together on the same plot given the disparity of scales.\n",
    "\n",
    "The syntax below does multiple things at the same time.\n",
    "\n",
    "> NOTE: `sklearn` expects data arrays to have the shape `(n_samples, n_features)`, so I may need to be a bit careful about how I jam the two arrays together. I avoid it completely by just reloading the relevant data, but the commented code shows another way (I can think of at least five other ways to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5af88d-d466-4982-a15f-6ae576885043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data\n",
    "data = df[[target_vars[ind[0]], target_vars[ind[1]]]].values\n",
    "print(f\"data shape is {data.shape}, already of right form\")\n",
    "print(\" \")\n",
    "\n",
    "# could also stack the two arrays together\n",
    "# X = df[target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "# Y = df[target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "# data = np.concatenate((X, Y), axis=-1)  # stack at the feature axis which is the last one\n",
    "\n",
    "# don't even bother defining the scaler object\n",
    "data_scale = StandardScaler().fit_transform(data)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# raw unscaled histograms\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.hist(data[:, 0], density=True, color=\"C0\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.hist(data[:, 1], density=True, color=\"C1\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$Y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.hist(data_scale[:, 0], density=True, color=\"C0\", alpha=0.7, label=r\"$\\tilde{X}$\")\n",
    "ax.hist(data_scale[:, 1], density=True, color=\"C1\", alpha=0.7, label=r\"$\\tilde{Y}$\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(r\"$\\tilde{X}$ and $\\tilde{Y}$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41839880-0967-4c82-88c1-5244352dd757",
   "metadata": {},
   "source": [
    "There are other ways to scale the data; see extended exercises later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f0b67-b6c7-443f-82e2-8d945b425c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe2e4d1e-2bf2-4b26-83be-bc3018406893",
   "metadata": {},
   "source": [
    "---\n",
    "## c) Model skill\n",
    "\n",
    "One more thing we need before talking cross-validation. If we regard ML as finding a model $f$ that maps inputs $X$ to outputs $Y$ (i.e. regression), then by the \"best model\" we mean some $f$ that minimises the ***mismatches*** between predictions $\\hat{Y}$ and $Y$. The problem then ultimately depends on what you define as the mismatch. A standard choice is the $L^p$ family of norms given by\n",
    "\\begin{equation*}\n",
    "    \\|\\hat{Y} - Y\\|_{L^p} = \\left(\\int |\\hat{Y} - Y|^p\\; \\mathrm{d}\\mu \\right)^{1/p}.\n",
    "\\end{equation*}\n",
    "The most commonly used ones of these are $L^2$ or ***mean squared error*** (MSE) and $L^1$ or ***mean absolute error*** defined by\n",
    "\\begin{equation*}\n",
    "    \\mathrm{MSE} = \\|\\hat{Y} - Y\\|^2_{L^2} = \\int |\\hat{Y} - Y|^2\\; \\mathrm{d}\\mu, \\qquad \\mathrm{MAE} = \\|\\hat{Y} - Y\\|_{L^1} = \\int |\\hat{Y} - Y|\\; \\mathrm{d}\\mu,\n",
    "\\end{equation*}\n",
    "where the integral is invariably replaced by sums in contexts encountered in this course.\n",
    "\n",
    "For the training of the ML model a choice of mismatch (i.e. the ***loss function***, or the ***objective function*** in the classical optimisation literature) would need to be specified, and $L^2$ or MSE is often used. On the other hand, we can also evalute a model's skill using MSE and MAE measures (or any other ones we think are relevant).\n",
    "\n",
    "> NOTE: Again, \"standard\" does not necessarily mean best.\n",
    ">\n",
    "> $\\mathrm{d}\\mu$ is related to a \"measure\"; not going to elaborate what that means except that it is a fundamental object in the theory of probability, and is more fundamental than the \"metric\".\n",
    ">\n",
    "> $L^2$ or MSE is often chosen to be the loss presumably because $L^2$ is differentiable and the resulting space is (presumably) a ***Hilbert space***, which has desirable properties. Not actually sure if this is the intention of most practicioners though...\n",
    "\n",
    "To see how this works we do need to choose a model, so I am going to choose Linear Regression (`LinearRegression` below) partly to demostrate syntax. Recall that linear regression does\n",
    "\\begin{equation*}\n",
    "    \\hat{Y} = aX + b,\n",
    "\\end{equation*}\n",
    "where the name of the game is to find $a$ and $b$ such that the $L^2$ mismatches are smallest. Since it is constructed to be a $L^2$ mminimiser we expect the related MSE will be small, but we also note this does not say anything about MAE. We are going to predict $Y$ from $X$; going to throw the whole data in for this one just because.\n",
    "\n",
    "> NOTE: Here I am not going to standardise the data because it doesn't matter for linear regression (because it's linear, so $a$ and $b$ change but it is just a rescaled linear line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c440c13-0ae4-42c6-bc18-ca6d1103b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# same syntax as above, (n_samples, n_features)\n",
    "X = df[target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(X, Y, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.plot(X, Y_pred, 'bx', label=\"predictions\")\n",
    "ax.set_xlabel(target_vars[ind[0]])\n",
    "ax.set_ylabel(target_vars[ind[1]])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"fitted model: Y = {model.coef_[0, 0]:.6f} X + {model.intercept_[0]:.3f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7595df-a133-47d2-9b76-d8a0cba58915",
   "metadata": {},
   "source": [
    "The model is expected to suck in terms of skill because the data is clearly not linear. Below compares the relevant errors as done through `sklearn` as well as a native way of doing this using `numpy`, as well as computing some other relevant statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dc305-ad2c-4ea3-b8f3-b44ddb211aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "# as computed by sklearn\n",
    "MSE_sklearn = mean_squared_error(Y, Y_pred)\n",
    "MAE_sklearn = mean_absolute_error(Y, Y_pred)\n",
    "\n",
    "# as computed by hand\n",
    "MSE_np = np.mean(np.abs(Y - Y_pred)**2)\n",
    "MAE_np = np.mean(np.abs(Y - Y_pred))\n",
    "\n",
    "print(f\"MSE from sklearn = {MSE_sklearn:.4f}\")\n",
    "print(f\"MSE from numpy   = {MSE_np:.4f}\")\n",
    "print(f\"MAE from sklearn = {MAE_sklearn:.4f}\")\n",
    "print(f\"MAE from numpy   = {MAE_np:.4f}\")\n",
    "print(\" \")\n",
    "print(f\"R^2 score of model = {model.score(X,Y):.4f}\")\n",
    "print(f\"correlation coeff of model = {r_regression(X, Y.ravel())[0]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3c465-deb2-44b9-97fb-bddb5aaada4c",
   "metadata": {},
   "source": [
    "Coinvince yourself the $R^2$ score is consistent that there is not really a linear correlation going on, and the negative linear correlation coefficient is also consistent with the overall shape of the data.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Do the above but scale the data, and convince yourself the same conclusions above effectively holds.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Check that the R^2 and linear correlation coefficient computed above coincide with that computed from `scipy` (cf. what was done in OCES 3301)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89747240-c87b-4970-b261-5e405037fa9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e8fe69c-97fd-4c7d-aee4-95ff9e13aa61",
   "metadata": {},
   "source": [
    "---\n",
    "## d) Validation and robustness\n",
    "\n",
    "Given the randomness in the data and train/test split, two things you may want to ask are:\n",
    "\n",
    "1. How robust is the model? (Did you get a model with good skill because you got lucky?)\n",
    "2. How generalisable is your model?\n",
    "\n",
    "ML models are generally prone to ***over-fitting***, in that it is easy to keep adding complexity into the model, which usually improves the skill over the training data, but fail miserably in the testing data. Going to illustrate a few things one could do to mitigate this a bit.\n",
    "\n",
    "If you go back to the first graph in the notebook where I coloured the data by `species`, you will notice that `Gentoo` is well-separated from `Adelie` and `Chinstrap`, so if I do a linear regression of those only those it will give me very different results. Below code demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65e9dc-cb85-4c59-8eea-7ca24290191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show difference of regression of Gentoo and (Adelie, Chinstrap)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "for species in np.sort(df[\"species\"].unique()):   # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_vars[ind[0]]], \n",
    "               df[df[\"species\"] == species][target_vars[ind[1]]],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "\n",
    "# fit gentoo data\n",
    "X = df[df[\"species\"] == \"Gentoo\"][target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[df[\"species\"] == \"Gentoo\"][target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "model = LinearRegression().fit(X, Y)\n",
    "ax.plot(X, model.predict(X), 'k--', label=f\"Gentoo LOBF, corr = {r_regression(X, Y.ravel())[0]:.4f}\")\n",
    "\n",
    "# fit other data (find NOT gentoos)\n",
    "X = df[df[\"species\"] != \"Gentoo\"][target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[df[\"species\"] != \"Gentoo\"][target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "model = LinearRegression().fit(X, Y)\n",
    "ax.plot(X, model.predict(X), 'm--', label=f\"Not Gentoo LOBF, corr = {r_regression(X, Y.ravel())[0]:.4f}\")\n",
    "\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "le = ax.legend()\n",
    "le.set_bbox_to_anchor([0.7, 0.8, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be634f6-8c5c-4668-8370-32f4c4d3e0f5",
   "metadata": {},
   "source": [
    "Notice the correlation of both models are positive (as it should be), and is of different sign to the correlation coefficient if all data is used. The example here is a bit contrived in that fitting to the whole data using a linear model is clearly not the best thing to do (given the clear separation by species), but it demonstrates the point that given the inherent randomness in data selection there is likely variations in the model that results. If the model depend sensitively on the data then that's probably not a good thing to have.\n",
    "\n",
    "Well just like we don't (at least we shouldn't) do one trial and conclude from it (unless there is strong reason to believe the system is strongly deterministic, e.g. some physical systems), then one way to investigate ***robustness/uncertainties*** in model is to do a few of these and take averages if need be. I am going to go further in this case and run a ton of `train_test_split`s, spit out the relevant model parameters, and plot the resulting histogram to get a sense of the pdf.\n",
    "\n",
    "> NOTE: I am effectively training an ***ensemble*** of models here. We will come back to ensembles in e.g. `06_rf_boosting` (***random forests*** is an ensemble method based on decision trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f405b-edeb-4895-a00c-703820d7eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a ton of trials of LinearRegression (Y = aX + b) but changing the train_test_split\n",
    "X = df[target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "\n",
    "# number of trials and initialise arrays to dump statistics in\n",
    "n = 100\n",
    "stats = np.zeros((n, 6))  # a, b, MSE on test data, MAE on test data, r, R2\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(n):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    model.fit(X_train, Y_train)\n",
    "    stats[i, 0], stats[i, 1] = model.coef_[0, 0], model.intercept_[0]\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    stats[i, 2] = mean_squared_error(Y_test, Y_pred)\n",
    "    stats[i, 3] = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "    stats[i, 4] = model.score(X_test, Y_test)\n",
    "    stats[i, 5] = r_regression(X_train, Y_train.ravel())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154bb34-8cf7-4dfe-80fb-0889d3b43a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the above statistics\n",
    "labels = [r\"$a$\", r\"$b$\", \"MSE\", \"MAE\", r\"$r$\", r\"$R^2$\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "for j in range(6):\n",
    "    ax = plt.subplot(2, 3, j+1)\n",
    "    ax.hist(stats[:, j])\n",
    "    ax.set_title(labels[j])\n",
    "    ax.grid()\n",
    "    if j % 3 == 0:\n",
    "        ax.set_ylabel(\"freq\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa98c7-0318-40ba-b097-3863db4a3228",
   "metadata": {},
   "source": [
    "You can compute the mean and standard deviations of these if you want to, but looking at the histogram the resulting model can be argued to be reasonably robust probably (this would be easier to see if the data was standardised actually). That's not surprising because linear regression is simple enough it probably can't do too many insane things. The same cannot be said of other models.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Do the above but scale the data, and convince yourself the same conclusions above effectively holds.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Compute the standard deviations and means (for raw or standardised data, or both if you really want to) and report the model values and statistics of those accordingly using appropriate `print` commands.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Explore what happens to the statistics reported above when you change the `test_size` value in `train_test_split`. (Have a think what you would expect if `test_size` were to ***increase*** substantially.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e954a-4c54-4b3b-86fc-04a0cae4405c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "685e133d-83c5-4d1d-9e89-4574f097f07e",
   "metadata": {},
   "source": [
    "---\n",
    "## e) Over-fitting\n",
    "\n",
    "To demonstrate ***over-fitting*** and thus the need for cross-validation I am going to do **polynomial** regression instead but using `np.polyfit` (could also this through `sklearn.preprocessing.PolynomialFeatures`). In this case my (only) ***hyperparameter*** of the model would be the degree of fitting; linear regression is fitting to a polynomial of degree 1. The goal is to choose one that balances skill with some degree of robustness: in this case I want (relatively) lower MSE/MAE scores, and maybe that the spread of the model statistics is not completely wild. \n",
    "\n",
    "I am going to fix the training/testing split to reduce the degree of possible variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46244a68-5513-485c-95e9-ab6b6173bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not going reshape because I don't need it when using np.polyfit\n",
    "\n",
    "X = df[target_vars[ind[0]]].values\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "\n",
    "# fix the seed for reproducibility purposes\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "n = 10\n",
    "MSE_train, MAE_train = np.zeros(n), np.zeros(n)\n",
    "MSE_test, MAE_test = np.zeros(n), np.zeros(n)\n",
    "for deg in range(n):\n",
    "    model = np.polyfit(X_train, Y_train, deg+1)\n",
    "    Y_pred = np.polyval(model, X_train)\n",
    "    MSE_train[deg] = mean_squared_error(Y_pred, Y_train)\n",
    "    MAE_train[deg] = mean_absolute_error(Y_pred, Y_train)\n",
    "    Y_pred = np.polyval(model, X_test)\n",
    "    MSE_test[deg] = mean_squared_error(Y_pred, Y_test)\n",
    "    MAE_test[deg] = mean_absolute_error(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588442a-4e04-4534-bc73-b2102f40c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(np.arange(1, n+1), MSE_train, \"C0-x\", label=\"MSE\")\n",
    "ax.plot(np.arange(1, n+1), MAE_train, \"C1-x\", label=\"MAE\")\n",
    "ax.set_xlabel(\"polynomial degree\")\n",
    "ax.set_title(\"training data\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(np.arange(1, n+1), MSE_test, \"C0-x\", label=\"MSE\")\n",
    "ax.plot(np.arange(1, n+1), MAE_test, \"C1-x\", label=\"MAE\")\n",
    "ax.set_xlabel(\"polynomial degree\")\n",
    "ax.set_title(\"testing data\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03234afd-66db-420e-9577-4c8497806c52",
   "metadata": {},
   "source": [
    "Above graph suggests that errors in the training are decreasing with increasing polynomial degree, although it is tapering off. This is not surprising as increased complexity generally allows for a better fit. On the other hand, the MSE error has reached some sort of minimum by degree 3. Both of these are symptoms of over-fitting, where the model skill increases with complexity but then fails to generalise as well to unseen data. \n",
    "\n",
    "> <span style=\"color:red\">Q.</span> In the above case I am not really extrapolating as such because of how the data is distributed, but if I were extrapolating by feeding it an $X$ beyond the possible range of values I provide for the training then it would probably start returning insane values. Show that this is in fact the case (you don't need to go that far beyond the total range if your polynomial degree is high).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Evaluate the robustness of the above conclusion to the choice of random seed. The result should suggest to you that you may or may not want to therefore compute averages somehow to get some sense of the \"optimal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e3b75-550e-4799-a649-768d6108db1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46f99e20-adf4-4620-9aa6-3ec3801db549",
   "metadata": {},
   "source": [
    "---\n",
    "## f) Cross-validation\n",
    "\n",
    "How do we go about selecting a model that has skill and is not too over-fitted? There isn't really one correct answer, but a main point is you probably need to multiple trains to evaluate dependence on the hyper-parameters etc., or take an average model of some sort.\n",
    "\n",
    "Training on all the data can be costly and one way to explore the dependence on hypoer-parameters is ***$k$-fold*** cross-validation:\n",
    "\n",
    "* Split data into train and test sets as usual\n",
    "* Further split training data set into $k$ groups, and train a model on all $k$ of those\n",
    "* From that, make a decision what it means to be a \"best\" model (e.g. take the best model from the batch of $k$ models, take some average of the resulting parameters that describe the model, others...)\n",
    "* Test the \"best\" model on the testing data set to evaluate over-fitting etc.\n",
    "\n",
    "Doing this does not avoid over-fitting as such, but it does test for ***sensitivity***, which is an important thing to do given the inherent randomness in the procedures. Below code chooses the \"best\" model by:\n",
    "\n",
    "1. The one with the lowest `MSE_test` score (where \"test\" means the ones from the $k$-fold split).\n",
    "2. The avergage of all the trained models.\n",
    "\n",
    "The we evaluate the skill on the withheld test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f0750-51ae-405f-bed3-0db3c7a91d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the procedure to split the training data further\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = df[target_vars[ind[0]]].values\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "# initialise a list and dump things in these\n",
    "k = 5\n",
    "deg = 5\n",
    "model = np.zeros((deg+1, k))\n",
    "MSE_train, MAE_train = np.zeros(k), np.zeros(k)  # don't actually use these but record anyway\n",
    "MSE_test, MAE_test = np.zeros(k), np.zeros(k)\n",
    "\n",
    "kf = KFold(n_splits=k)\n",
    "i = 0\n",
    "\n",
    "# only do k-fold split on the TRAINING data (the model still never sees the TESTING data)\n",
    "for train, test in kf.split(X_train):  # these generate a bunch of indices\n",
    "    model[:, i] = np.polyfit(X_train[train], Y_train[train], deg)\n",
    "    MSE_train[i] = mean_squared_error(np.polyval(model[:, i], X_train[train]), Y_train[train])\n",
    "    MAE_train[i] = mean_absolute_error(np.polyval(model[:, i], X_train[train]), Y_train[train])\n",
    "    MSE_test[i] = mean_squared_error(np.polyval(model[:, i], X_train[test]), Y_train[test])\n",
    "    MAE_test[i] = mean_absolute_error(np.polyval(model[:, i], X_train[test]), Y_train[test])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5101c-492a-4fcb-afa8-48ff80a05aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model from lowest MSE_test, and from an average of those models\n",
    "model_best1 = model[:, np.where(MSE_test == np.min(MSE_test))[0][0]]\n",
    "model_best2 = np.mean(model, axis=-1)  # averaging over the \"model\" dimension\n",
    "\n",
    "# here we are passing (X_test, Y_test) in\n",
    "MSE_best1 = mean_squared_error(np.polyval(model_best1, X_test), Y_test)\n",
    "MSE_best2 = mean_squared_error(np.polyval(model_best2, X_test), Y_test)\n",
    "MAE_best1 = mean_absolute_error(np.polyval(model_best1, X_test), Y_test)\n",
    "MAE_best2 = mean_absolute_error(np.polyval(model_best2, X_test), Y_test)\n",
    "\n",
    "print(f\"model_best1 has:\")\n",
    "print(f\"  coeffs = {model_best1}\")\n",
    "print(f\"  MSE    = {MSE_best1:.6f}\")\n",
    "print(f\"  MAE    = {MAE_best1:.6f}\")\n",
    "print(\" \")\n",
    "print(f\"model_best2 has:\")\n",
    "print(f\"  coeffs = {model_best2}\")\n",
    "print(f\"  MSE    = {MSE_best2:.6f}\")\n",
    "print(f\"  MAE    = {MAE_best2:.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757999e-c6a0-47cb-bf60-9c6a522cf5fe",
   "metadata": {},
   "source": [
    "Several things to be read from the above is that:\n",
    "\n",
    "* The best model has lower MSE, but the \"averaged\" model has marginally better performance in MAE, so which is better is somewhat arguable...\n",
    "* The resulting models are largely the same and has a large value in the last entry, in this case the leading coefficient of $X^5$. (This is probably not a good thing regarding over-fitting...)\n",
    "\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Play around with the choice of `k` and `deg`. I would expect the leading coefficient to still be large, which would indicate severe sensitivity if we are doing extrapolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804ac2b-f141-435e-99df-aead7659160d",
   "metadata": {},
   "source": [
    "The thing to note is over-fitting is potentially more subtle in other models, so it is prudent to do ensembles and/or cross-validate; you should basically assume the ML models you create are probably over-fitted in general, and provide evidence to the degree of over-fitting as appropriate. Later notebooks will demonstrate cross validation and testing for robustness sporadically, but the comprehensive exploration will largely be left as exercises.\n",
    "\n",
    "> NOTE: There is an easier function `model_selection.cross_val_score` that could be used, but that requires an sklearn estimator object and doesn't play too well here for linear regression (partly because `LinearRegression` is too simple). We will use that briefly when we come to `03_lin_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a64ee-fe9b-47e0-a4db-21244a5a7882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf906c3-8b40-461f-bbb0-6426baf3999a",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Comments on over-fitting and robustness of model is expected of all the assignments that are to be handed in.</span> \n",
    "1. A portion of the marks under \"coding\" and marks under \"science\" will be given for (cross-)validation and related evaluations for model skill and robustness/sensitivity to hyper-parameters, data splitting etc., so you automatically lose quite a lot of marks if you don't do it (cf. referencing: no one likes doing it as such, but it needs to be done).\n",
    "3. Saying you've done (cross-)validation but without providing numerical evidence will be treated as if no (cross-)validation was in fact performed.\n",
    "4. An ensemble calculation of about 5 to 10 would be minimally expected (given you are not expected to be creating models that are large and thus slow to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536d483-2729-48fb-9b02-6bb489889874",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Use of `pipeline`\n",
    "\n",
    "#### (May help with all upcoming assignments.) \n",
    "\n",
    "A whole load of the things above can be packaged in one go using [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Redo some of the above using those instead; this may be useful to package up for assignments later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d26a7-2313-42ee-b252-dc438fedec4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8dd9-8d3a-4baf-94d5-ad2c72d20bda",
   "metadata": {},
   "source": [
    "## 2) Other ways of scaling\n",
    "\n",
    "Have a look at [here](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) and explore other ways of scaling the data.\n",
    "\n",
    "You could also try scaling things according to non-Gaussian pdfs, but you may need to do this yourself. Could also consider ***non-dimensionalisation*** based on specific processes (e.g. if you've done my OCES 2003 you would have seen things like the Rossby/Reynolds/Rayleigh/Ekman number; this may show up in the extra material of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36a690-cfe0-4ce5-935d-f26579d9cdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c213119-8eb2-4cb3-8190-0b8b33a71544",
   "metadata": {},
   "source": [
    "## 3) $L^1$ minimising linear regression\n",
    "\n",
    "This was previously posted as a (hard-ish) problem to do in OCES 3301. Linear regression by default do $L^2$ (mean squared error or MSE) minimisation, but that says nothing about the $L^1$ (mean absolute error or MAE), or indeed other norms. See how you would create a $L^1$ minimising linear regression predictor, and show the resulting object does seem to minimise $L^1$, reduces weighting on the outliers, but says nothing about the $L^2$ or other errors.\n",
    "\n",
    "I would do it (and have done it beofre) through `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc2dce-4b93-4a46-8ed8-89b2e4c30457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
