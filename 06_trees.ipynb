{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f25e79-2207-4ab6-a7fe-53f98e84d12c",
   "metadata": {
    "id": "47f25e79-2207-4ab6-a7fe-53f98e84d12c"
   },
   "source": [
    "*updated 08 Aug 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 4303 \"AI and Machine Learning in Marine Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES4303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f3a7e-65dc-4a63-98a8-1bc11ba2fe91",
   "metadata": {
    "id": "673f3a7e-65dc-4a63-98a8-1bc11ba2fe91"
   },
   "source": [
    "---\n",
    "# 6. Decision trees\n",
    "\n",
    "An example of a ***decision tree*** is as below:\n",
    "\n",
    "<img src=\"https://i.imgur.com/UayMNei.png\" width=\"450\" alt='cat decision tree'>\n",
    "\n",
    "From a manual point of view it's not too hard to see how we create decision trees, but how would a machine do this? The goal of this notebook is to spend a bit of time introducing some concepts behind how a single decision tree is created, and how you might use these as classifiers and/or regressors. This sets the scene for the next notebook: ***random forest*** and ***gradient boosting*** approaches are effective an ensemble of trees.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Go into a little detail about the anatomy of decision trees, and how trees are created by the machine (need a bit of probability here)\n",
    "> 2. Consider an example of decision tree as a classifier and a regressor\n",
    "> 3. Some procedures to prevent over-fitting (e.g. pruning)\n",
    "\n",
    "We are going to admire this picture of a tree for a bit, then load some packages and go through some details relating to how decision trees are created. For this notebook I am going to use the penguins data for the whole way (it's actually easier to make my points with it).\n",
    "\n",
    "<img src=\"https://i.imgur.com/2MeMlCK.jpeg\" width=\"450\" alt='broc-collie'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b5105-99b3-4f70-b453-d2113a685a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# load the penguin data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES4303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66137c-dc5f-4801-b774-d5565390b410",
   "metadata": {
    "id": "6b66137c-dc5f-4801-b774-d5565390b410"
   },
   "source": [
    "---\n",
    "## a) Some background\n",
    "\n",
    "### Anatomy of a decision tree\n",
    "\n",
    "Going to introduce some terminology so I can use it later:\n",
    "\n",
    "* Root: This would be the start of the tree (the \"is that for me?\" box above)\n",
    "* Leaves: The things at the end (the \"I don't want it\" and \"That's for me\" box above)\n",
    "* Node: The intermediate blocks connecting the root to the leaves (in this case strictly speaking there isn't one; leaves can be pure nodes with no subsequent connectors)\n",
    "* Branches: The connectors detailing the decisions (here it really should be the \"yes\" and \"no\")\n",
    "* Levels/depth: Distance from the node and maximum number of levels (I would call above depth 1, arguably it could be 2)\n",
    "* Parent/child: Usually refer to nodes that are directly connected. If a tree is going down like the above, the parent node would be the higher one (at a *lower* level; here the root node is the parent and the leaves are the child nodes)\n",
    "\n",
    "### Recap in probability, and the concept of information entropy\n",
    "\n",
    "(I want to spend a bit of time on this because similar ideas crop out when talking about Neural Networks.)\n",
    "\n",
    "Recall that a ***probability*** $p_i$ is a value between 0 to 1 assigned to an event $X_i$ occuring, with the condition that the sum of all probabilities should be 1, i.e. $\\sum p_i = 1$. With the penguin data, this might be the probability of me randomly picking a sample out of the full dataset and the chances of me picking `Adelie`, `Chinstrap` or `Gentoo`. I am going to assume the process of drawing a sample is fair (i.e. follows a uniform distribution), then the probabilities are simply the number of samples of $X_i$ divided by the total sample size $N$. Let's actually do that in code form (because I want to use the result later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ec0e6-fa95-4541-84ec-d5b46c9d6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of drawing a particular species\n",
    "\n",
    "# pandas can actually do this in one go\n",
    "freq = df[\"species\"].value_counts()\n",
    "p = df[\"species\"].value_counts(normalize=True)  # pandas can actually do this in one go\n",
    "\n",
    "print(freq)\n",
    "print(f\"total number of samples = {np.sum(freq)}\")\n",
    "print(\" \")\n",
    "print(p)\n",
    "print(f\"total probability = {np.sum(p)}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0dab5-3dc1-44be-bc49-3c287622570c",
   "metadata": {
    "id": "46b0dab5-3dc1-44be-bc49-3c287622570c"
   },
   "source": [
    "Thing to notice for later is that the dataset is unbalanced, with fewer samples in `Chinstrap`.\n",
    "\n",
    "If instead you want to find the probability in picking out a continuous variable like `bdoy_mass_g`, then it doesn't make sense to talk about a probability of picking out a sample that has 4000 g, because it is almost surely not going to happen. Instead you talk about the probability of picking out a sample between (say) 3900 and 4100 g, i.e. you create ***bins*** or categories for the data to fall into, and then you proceed as before. This is of course just the same as creating a histogram, and in this case it is the area of the histogram (or pdf) that should sum (or integrate) to 1.\n",
    "\n",
    "A quantity of interest is called the ***information entropy*** or the ***Shannon entropy***, defined as\n",
    "\\begin{equation*}\n",
    "    H = \\sum_{i=0}^N H_i = -\\sum_{i=0}^N p_i \\log_a p_i,\n",
    "\\end{equation*}\n",
    "where I have not been very specific about the choice of base $a$. The original definition uses $a=2$, although $a=10$ and $a=e$ (so $\\log \\to \\ln$) will also work.\n",
    "\n",
    "> NOTE: It doesn't really matter because almost all the time it's the \"shape\" or functional behaviour of $H$ that is of importance, rather than it's value. I am actually going to use the natural log (i.e. $a=e$), and I am simply going to denote that $\\log$ (rather than use $\\ln$). You can call the related logs in `numpy` via `np.log2`, `np.log10` and `np.log`.\n",
    "\n",
    "This quantity shows up in ecology as the Shannon index and is supposed to be a measure of species diversity. The way you may want to think about entropy is the amount of surprise or information you get from doing a random sample from the dataset: $H=0$ is no surprise and perfect information, while high $H$ means the converse. To see this, consider the two extremes:\n",
    "\n",
    "* If there is only one species, so $p_0 = 1$ by construction, but $\\log 1 = 0$ for any base, so $H = 0$. Drawing a sample here provides zero surprise and information, because you can't get any other possibility anyway (at least for this category).\n",
    "* Convince yourself that $H$ is maximised for a uniform distribution, in which case $p_i = 1/(N+1)$, so $\\log 1/(N+1) = \\log 1 - \\log (N+1) = -\\log(N+1)$ for any base, so $H = (N+1)/(N+1)\\times \\log(N+1) = \\log(N+1)$. Diversity is thus maximum, and drawing a sample is maximally surprising because there is no bias in the dataset.\n",
    "\n",
    "The penguins data is not balanced so we could calculate the Shannon index for it (although I am using the wrong base on purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41c345-157b-45b0-b940-9d043e05caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shannon index for all penguin data based on species\n",
    "\n",
    "H = (-p * np.log(p)).sum()\n",
    "print(f\"H = {H:.6f} in base e\")\n",
    "\n",
    "# pick out number of unique entries\n",
    "num_unique = len(df[\"species\"].unique())\n",
    "print(f\"maximum possible Shannon entropy = {np.log(num_unique):.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1b9e5-75d5-4588-8c37-beed3a73cfe3",
   "metadata": {
    "id": "a6d1b9e5-75d5-4588-8c37-beed3a73cfe3"
   },
   "source": [
    "We can play the same game for continuous data; going to do this for the case `body_mass_g` feature as it serves to be an intermediate step to the next part in terms of some `pandas` syntax. The first part shows which bins it's actually going to dump the data in, and the second does the binning and computation of the probabilities, then entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3e22e-22ef-4818-8518-c2f4da2eb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the data and return bins\n",
    "out = pd.cut(df[\"body_mass_g\"], 5)\n",
    "bins = out.unique()  # find the number of unique entries based on intervals\n",
    "print(out.value_counts())\n",
    "print(\" \")\n",
    "\n",
    "p = out.value_counts(normalize=True)\n",
    "H = (-p * np.log(p)).sum()\n",
    "print(f\"H = {H:.6f} in base e\")\n",
    "\n",
    "# pick out number of unique entries\n",
    "num_unique = len(bins)\n",
    "print(f\"maximum possible Shannon entropy = {np.log(num_unique):.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edeec9-22ad-4b2d-af1a-6327915f85e5",
   "metadata": {
    "id": "14edeec9-22ad-4b2d-af1a-6327915f85e5"
   },
   "source": [
    "### Information gain\n",
    "\n",
    "So the value of the entropy by itself is neither here nor there. The question to ask here is that suppose I already chose a feature to segment the data (say `species`), then given a choice of choosing the next feature to segment data on, which one should I choose? One way to do this is to choose the feature that gives you the maximum ***information gain***: if high entropy is low information, I want to choose the feature that lowers the resulting entropy as much as possible. Information gain is calculated as\n",
    "\\begin{equation*}\n",
    "    \\mbox{IG} = H_p - \\sum_{i=0}^N p_{c,i} H_{c,i},\n",
    "\\end{equation*}\n",
    "where $H_p$ is the entropy of the parent class, and $p_c$ and $H_c$ are the probabilities and entropy associated with the child classes.\n",
    "\n",
    "> NOTE: IG can be written in terms of the ***Kullback-Liebler*** (KL) divergence, which is an important concept in information geometry and used quite widely in ML. Not going to elaborate on what that is though; think of it as a distance between pdfs.\n",
    "\n",
    "Probably easier to explain with an example:\n",
    "\n",
    "1. If we take `species` as the parent feature, then we already calculated $H_p$ once above.\n",
    "2. If I take `body_mass_g` as the child feature to further segment on, then I need to bin the data accordingly (also done above).\n",
    "3. Find the number of `species` in those `body_mass_g` bins, and compute the occurrences and entropies accordingly. The below does this by creating the bins, finding the indices corresponding to the unique bins, pick out the data, and then computing the entropies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac4581-1196-41bc-99ac-3fc567d300fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing IG\n",
    "\n",
    "# compute entropy of parent first\n",
    "p = df[\"species\"].value_counts(normalize=True)\n",
    "H = (-p * np.log(p)).sum()\n",
    "\n",
    "# compute intermediate variables\n",
    "out = pd.cut(df[\"body_mass_g\"], 5)\n",
    "bins = out.unique()\n",
    "\n",
    "H_i, n_i = np.zeros(len(bins)), np.zeros(len(bins))  # using n later to compute p_i\n",
    "for i in range(len(bins)):\n",
    "    ind = (out == bins[i])\n",
    "    n_i[i] = df[ind][\"species\"].value_counts().sum()\n",
    "    p_i = df[ind][\"species\"].value_counts(normalize=True)\n",
    "    H_i[i] = (-p_i * np.log(p_i)).sum()\n",
    "\n",
    "# compute information gain\n",
    "IG = H - np.sum(H_i * n_i / np.sum(n_i))  # n_i / sum(n_i) = p_i\n",
    "print(f\"IG(parent = species, child = body_mass_g) = {IG:.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6da0b8-24c6-40d2-8775-49cd7ab5d70d",
   "metadata": {
    "id": "9b6da0b8-24c6-40d2-8775-49cd7ab5d70d"
   },
   "source": [
    "Positive IG here means the entropy is being reduced, which is expected. We should then check this for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3323b2-2ee2-493f-96c1-e59f5b8c4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute IGs\n",
    "\n",
    "# compute entropy of parent first\n",
    "p = df[\"species\"].value_counts(normalize=True)\n",
    "H = (-p * np.log(p)).sum()\n",
    "\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "\n",
    "for child_feature in feature_names:\n",
    "\n",
    "    # compute intermediate variables\n",
    "    out = pd.cut(df[child_feature], 5)\n",
    "    bins = out.unique()\n",
    "\n",
    "    H_i, n_i = np.zeros(len(bins)), np.zeros(len(bins))  # using n later to compute p_i\n",
    "    for i in range(len(bins)):\n",
    "        ind = (out == bins[i])\n",
    "        n_i[i] = df[ind][\"species\"].value_counts().sum()\n",
    "        p_i = df[ind][\"species\"].value_counts(normalize=True)\n",
    "        H_i[i] = (-p_i * np.log(p_i)).sum()\n",
    "\n",
    "    # compute information gain\n",
    "    IG = H - np.sum(H_i * n_i / np.sum(n_i))\n",
    "    print(f\"IG(parent = species, child = {child_feature:<17}) = {IG:.6f}\")\n",
    "\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e40ff-adb7-4e37-93ac-c9d84e43562c",
   "metadata": {
    "id": "051e40ff-adb7-4e37-93ac-c9d84e43562c"
   },
   "source": [
    "Note that the IG is the largest for `flipper_length_mm`, i.e. it's reducing the entropy the most. It is then suggesting that, given we chose `species` as a parent, we probably want to segment next on `flipper_length_mm`. Thus this is one measure on how the entropy can guide the creation of branches and leaves in decision trees.\n",
    "\n",
    "> NOTE: This is all relative and ordering matters: choosing `flipper_length_mm` as parent feature then `species` as child feature will not give the same IG.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Modify code accordingly to include both continuous and discrete labels as parent/child features to come for the IG. I would do the continuous ones first before I do the discrete ones; you probably need to convert the labels to numerical values first, and then dump that into the `df` as part of a `pandas` dataframe so you can use the subroutines above.\n",
    "\n",
    "To close, there is a similar quantity to the information entropy called the ***Gini index*** or ***Gini impurity***, which instead has the form\n",
    "\\begin{equation*}\n",
    "    G = \\sum_i p_i(1 - p_i).\n",
    "\\end{equation*}\n",
    "The Gini index measures \"impurities\" in the dataset: the case with a single feature would have $p_i = 1$, so $G = 0$ and is completely \"pure\", and corresponds also to the $H=0$ case. $G=1$ would be the maximally mixed case. Both can be used to constructu something like an information gain measure like the above, although the Gini index is more computationally efficient for binary decisions such as the trees we are considering. Information entropy and related ideas have more links with neural networks (hence me spending more time on that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeee645-1198-4cbe-aea6-efb47650b96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "134d7ee1-dcda-4aaa-a90c-39e23de583fe",
   "metadata": {
    "id": "134d7ee1-dcda-4aaa-a90c-39e23de583fe"
   },
   "source": [
    "---\n",
    "## b) Trees as classifiers\n",
    "\n",
    "So for classifiers we are going to take target $Y$ to the `species` feature. Going to load a whole load of things and to demonstrate some features that one could dig into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3a1dc-8e7d-4f41-ac58-b637f7dbd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df[\"species\"])\n",
    "\n",
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4167)\n",
    "# JL: Same as before; normalisation needs to come after train test split\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba15b93-997f-453a-88ab-44e70d94d69c",
   "metadata": {
    "id": "7ba15b93-997f-453a-88ab-44e70d94d69c"
   },
   "source": [
    "Going to fit a deliberately shallow tree (by specifying `max_depth=2`), and force it to use the information entropy criterion with `criterion=\"entropy\"`. I am also going to query the feature importance in the fitted model with `model.feature_importances_`.\n",
    "\n",
    "> NOTE: I am overriding the defaults are `max_depth=None` and `criterion=\"gini\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346611e-5984-4348-ac1e-f65a53230432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a decision tree (deliberately shallow for showing)\n",
    "model = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\")\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b3786-e2ef-4c10-bb07-71cac19c0fe9",
   "metadata": {
    "id": "099b3786-e2ef-4c10-bb07-71cac19c0fe9"
   },
   "source": [
    "The model does pretty well on the testing dataset. It also seems to be finding that `flipper_length_mm` is the most important feature, which is potentially consistent with our IG calculation above.\n",
    "\n",
    "Note also particular (on mine at least) it doesn't actually use the `body_mass_g` feature at all, because the `body_mass_g` has zero feature importance. This is partly because I forced `max_depth=2` (so I can't have that much complexity anyway), and it basically managed to classify all the data with three features. This would be an early termination criterion: if all data is accounted for, stop any further creation of nodes.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Why am I only using \"potentially\"? What is wrong with my assertion/statement here? (How is IG defined?)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Convince yourself that for this case it doesn't matter really if I scale the data or not. In other words, decision trees are ***scale-invariant*** (but I am still scaling because it possibly lessens the computational burden a bit).\n",
    "\n",
    "The thing with decision trees (and somewhat unlike most neural networks) you could actually look at the tree that gets formed, and is arguably interpretable, although of course if your model is sufficiently complex that may be a mute point anyway. In the above I deliberately forced the tree to be simple so the display coming up is a bit easier to see. I can use `plot_tree` to actually plot a figure, and `export_text` to spit out the entries in the decision tree.\n",
    "\n",
    "> NOTE: I passed in the `features_names=` argument so that it would display the feature names, otherwise it would display $X[2]$ etc., which refers to the third feature (in this case because python counts from zero), which is `flipper_length_mm`.\n",
    ">\n",
    "> The colours in the trees denote the classes, as can be seen from the text description of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b575d97-0cbc-4934-b9c0-e5345ab43a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, export_text\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "plot_tree(model, filled=True, proportion=True, feature_names=feature_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff0c4d-1b26-40e0-8fd8-87e1521f2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as text\n",
    "print(export_text(model, feature_names=feature_names))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e7fb2-4ab0-4866-8be3-7693f6461857",
   "metadata": {
    "id": "948e7fb2-4ab0-4866-8be3-7693f6461857"
   },
   "source": [
    "If we do not force the maximum depth then we get the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e0ec0-f159-479d-aa80-2ac467a48263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a decision tree (deliberately shallow for showing)\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")\n",
    "\n",
    "print(export_text(model, feature_names=feature_names))\n",
    "print(\" \")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "plot_tree(model, filled=True, proportion=True, feature_names=feature_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff4c2e-1a9c-4abc-a2e6-59f13fc111c7",
   "metadata": {
    "id": "b5ff4c2e-1a9c-4abc-a2e6-59f13fc111c7"
   },
   "source": [
    "Note in this case class 2 (which is `Gentoo`) has a termination node that is quite high up on the tree close to the root node. If you remember this is consistent with our observations that `Gentoo` data seems to live in a slightly different part of the feature space compared to the other two classes.\n",
    "\n",
    "### Decision boundaries\n",
    "\n",
    "We can plot out the decision boundaries easily if we only models using two features. Below takes three specific combinations just to demonstrate what these look like.\n",
    "\n",
    "> NOTE: Note I've standardised the data so the units are not quite right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8e0d4-b48b-47e9-aab8-8651fc8a96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the decision boundary\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "combos = [[1, 0], [2, 0], [3, 0]]  # pre-define some pairs of features\n",
    "colors = \"ryb\"  # red yellow blue\n",
    "\n",
    "fig = plt.figure(figsize=(9, 3))\n",
    "\n",
    "for i in range(3):\n",
    "    # train a new model per cycle\n",
    "    model = DecisionTreeClassifier().fit(X_train[:, combos[i]], Y_train)\n",
    "\n",
    "    ax = plt.subplot(1, 3, i+1)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        X_train[:, combos[i]],\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        ax=ax\n",
    "        )\n",
    "    # plot the actual data points in\n",
    "    for j in range(3): # 3 species\n",
    "        idx = np.where(Y_train == j)\n",
    "        ax.scatter(X_train[idx, combos[i][0]], X_train[idx, combos[i][1]],\n",
    "                   c=colors[j], edgecolor=\"k\", s=10, label=f\"{encoder.classes_[j]}\")\n",
    "    ax.set_xlabel(f\"{feature_names[i+1]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(f\"{feature_names[0]}\")\n",
    "ax.legend()\n",
    "\n",
    "fig.suptitle(r\"boundaries based on pairs of features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038c66a-ce84-4e66-9724-d84a83a982b7",
   "metadata": {
    "id": "3038c66a-ce84-4e66-9724-d84a83a982b7"
   },
   "source": [
    "### Dependence on `max_depth`\n",
    "\n",
    "If we don't specify `max_depth` then the trees goes on until the leave nodes are \"pure\", so either entropy or Gini index is zero. It is sometimes a good idea to limit `max_depth` as a control to possiblities of over-fitting. Below code changes that parameter to see how the skill varies as a function of `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1ab92-8148-4bf2-b18b-dd29a48bf9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependence of skill with varying max_depth\n",
    "\n",
    "for max_depth in range(1, 6):\n",
    "    # fit a decision tree with fixed seed\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=167)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # basic skill diagnostics\n",
    "    Y_pred = model.predict(X_test)\n",
    "    N = len(Y_test)\n",
    "    skill_all = np.sum(Y_pred == Y_test)\n",
    "    print(f\"max_depth = {max_depth} overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "    print(f\"fitted feature importance [1 = max]:\")\n",
    "    for i in range(len(feature_names)):\n",
    "        print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d680f7-f706-4a0f-af40-5fd71028eb2b",
   "metadata": {
    "id": "e4d680f7-f706-4a0f-af40-5fd71028eb2b"
   },
   "source": [
    "It looks like the model is reaching it's peak performance around `max_depth=2` or `max_depth=3`.\n",
    "\n",
    "### Dependence on `criterion`\n",
    "\n",
    "Below does the same as above but using the (default) Gini criterion instead. Performance ends up being about similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22215c75-61b0-41f3-a5de-5137a6f24de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependence of skill with varying max_depth\n",
    "\n",
    "for max_depth in range(1, 6):\n",
    "    # fit a decision tree with fixed seed\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=167)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # basic skill diagnostics\n",
    "    Y_pred = model.predict(X_test)\n",
    "    N = len(Y_test)\n",
    "    skill_all = np.sum(Y_pred == Y_test)\n",
    "    print(f\"max_depth = {max_depth} overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "    print(f\"fitted feature importance [1 = max]:\")\n",
    "    for i in range(len(feature_names)):\n",
    "        print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d42ed-e466-4e97-a64f-e41af4d74241",
   "metadata": {
    "id": "b41d42ed-e466-4e97-a64f-e41af4d74241"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> There are a whole load of other parameters one could specify for decision trees (e.g. see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)). Have a play around with those.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> I can think you pass further options into `plot_tree` to beautify it accordingly, have a look at doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972e6d4-f389-47d4-a3e1-29245c24d28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a91ca34-b6ed-43ba-85f1-0c947591f6dc",
   "metadata": {
    "id": "1a91ca34-b6ed-43ba-85f1-0c947591f6dc"
   },
   "source": [
    "---\n",
    "## c) Trees as regressors\n",
    "\n",
    "Recall from that example in session 2 for penguins data encountered, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e055bb5-1af1-41ad-8af1-fe8c66467400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 2d plot of two specific variables in the penguins data\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "ind = [3, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "for species in np.sort(df[\"species\"].unique()): # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_vars[ind[0]]],\n",
    "               df[df[\"species\"] == species][target_vars[ind[1]]],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a61eb-43cd-4867-aca3-607388498719",
   "metadata": {
    "id": "0b0a61eb-43cd-4867-aca3-607388498719"
   },
   "source": [
    "So here if I do standard linear models on the full data it's pretty bad, because there are clearly clusters identified by the `species` feature, but you can't really put that information into those regression models. Here we explore the possibility of doing that with decision trees. Below loads the `DecisionTreeRegressor` object and defines the input and output data accordingly: noticed I changed my `X` and `Y`s, and I had to do some funny things to make sure the data has the right shapes.\n",
    "\n",
    "> NOTE: The regressor now uses a different loss function, because the aim is slightly different now. The branching aspect is similar though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1516c-5ec1-4d76-b8c0-cd6ed8ac3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# standardise first but overwrite the inputs accordingly\n",
    "feature_names = [\"bill_depth_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "# X = StandardScaler().fit_transform(df[feature_names].values)\n",
    "\n",
    "# define target first (want the 1st feature \"flipper_length_mm\" here)\n",
    "Y = X[:, 0]\n",
    "\n",
    "# define the species feature and then stack it with \"body_mass_g\"\n",
    "encoder = LabelEncoder()\n",
    "dummy = encoder.fit_transform(df[\"species\"])\n",
    "X = np.concatenate((X[:, 1].reshape(-1, 1), dummy.reshape(-1, 1)), axis=-1)\n",
    "\n",
    "print(f\"input X has shape = {X.shape}\")\n",
    "print(\" \")\n",
    "\n",
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4167)\n",
    "# JL: Same as above, scale the first col, (becoz second col is label, i.e. int)\n",
    "X_column_trans = ColumnTransformer(\n",
    "    [('scaler', StandardScaler(), [0,])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "scalar_y = StandardScaler()\n",
    "X_train = X_column_trans.fit_transform(X_train)\n",
    "X_test = X_column_trans.transform(X_test)\n",
    "Y_train = scalar_y.fit_transform(Y_train.reshape(-1,1)).ravel()\n",
    "Y_test = scalar_y.transform(Y_test.reshape(-1,1)).ravel()\n",
    "\n",
    "\n",
    "# fit a decision tree (deliberately shallow for showing)\n",
    "model = DecisionTreeRegressor(max_depth=2, criterion=\"squared_error\")\n",
    "model.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047fe09-8413-46c8-ab22-63e8bb4af425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the tree\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "plot_tree(model, filled=True, proportion=True, feature_names=[\"body_mass_g\", \"species\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a7a38-b6df-4432-ae4d-b6222a5c25a6",
   "metadata": {
    "id": "847a7a38-b6df-4432-ae4d-b6222a5c25a6"
   },
   "source": [
    "The model is deciding in this case it would like to split according to species first, which is not unexpected. We test for the skill below on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67628eb6-4a6c-42ba-972b-13eca91d1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a prediction\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = plt.axes()\n",
    "for j in range(len(encoder.classes_)):\n",
    "    ax.scatter(X_test[X_test[:, 1]==j, 0], Y_test[X_test[:, 1]==j],\n",
    "               label=f\"{encoder.classes_[j]} true\",\n",
    "               color=f\"C{j}\",\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "    ax.scatter(X_test[X_test[:, 1]==j, 0], Y_pred[X_test[:, 1]==j],\n",
    "               label=f\"{encoder.classes_[j]} predict\",\n",
    "               color=f\"C{j}\",\n",
    "               marker=\"x\",\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0050c7-cba5-40d1-aceb-a3dac4fcd3ac",
   "metadata": {
    "id": "fb0050c7-cba5-40d1-aceb-a3dac4fcd3ac"
   },
   "source": [
    "So in this case one single model is able to ingest the `species` feature to do regression. This is in contrast to the previous suggestion where you either use existing labels or create labels with clustering, then you train up a model depending on the number of labels.\n",
    "\n",
    "The predictions are not fantastic in this case and clearly piecewise constant, and that's because I've specified a small vaule of `max_depth` so I can show the decision tree. With increased `max_depth` it can be seen we recover variability in the dataset accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5d0da-70d5-4932-ab98-a1ad86a8d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no restriction on depth\n",
    "model = DecisionTreeRegressor(criterion=\"squared_error\")\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = plt.axes()\n",
    "for j in range(len(encoder.classes_)):\n",
    "    ax.scatter(X_test[X_test[:, 1]==j, 0], Y_test[X_test[:, 1]==j],\n",
    "               label=f\"{encoder.classes_[j]} true\",\n",
    "               color=f\"C{j}\",\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "    ax.scatter(X_test[X_test[:, 1]==j, 0], Y_pred[X_test[:, 1]==j],\n",
    "               label=f\"{encoder.classes_[j]} predict\",\n",
    "               color=f\"C{j}\",\n",
    "               marker=\"x\",\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c7b97-eedf-493d-b6f9-d85223a1dc95",
   "metadata": {
    "id": "e47c7b97-eedf-493d-b6f9-d85223a1dc95"
   },
   "source": [
    "### Pruning\n",
    "\n",
    "Pruning refers to removing nodes accordingly. You do this with plants to keep the shape and/or not let the whole plant grow wildly. For the context here you want to do it as a way to control over-fitting by simplifying the tree. The measure of interest is that some kind of balance between skill and complexity (cf. AIC and BIC): you want to remove the nodes that contribute complexity but not so much in skill. The practical way of doing this would be to specify the `ccp_alpha` parameter to the tree initialisation. Below I consider the full depth case and compare a case with `ccp_alpha` zero and one with small. The things I am looking at particularly is the complexity of the resulting tree, which I can call from the `model` created.\n",
    "\n",
    "> NOTE: Similar ideas are used in neural networks where nodes in the layers can be dropped accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee95a10-4801-40e4-b758-c706460931b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out some statistics of the tree\n",
    "model = DecisionTreeRegressor(criterion=\"squared_error\")\n",
    "model.fit(X_train, Y_train)\n",
    "print(f\"full depth model has depth = {model.get_depth()} and {model.get_n_leaves()} leaves\")\n",
    "Y_pred = model.predict(X_test)\n",
    "print(f\"full depth model has MSE = {mean_squared_error(Y_pred, Y_test):.4f}\")\n",
    "\n",
    "model = DecisionTreeRegressor(ccp_alpha=0.001, criterion=\"squared_error\")\n",
    "model.fit(X_train, Y_train)\n",
    "print(f\"pruned model has depth = {model.get_depth()} and {model.get_n_leaves()} leaves\")\n",
    "Y_pred = model.predict(X_test)\n",
    "print(f\"pruned model has MSE = {mean_squared_error(Y_pred, Y_test):.4f}\")\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = plt.axes()\n",
    "for j in range(len(encoder.classes_)):\n",
    "    ax.scatter(X_test[X_test[:, 1]==j, 0], Y_test[X_test[:, 1]==j],\n",
    "               label=f\"{encoder.classes_[j]} true\",\n",
    "               color=f\"C{j}\",\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "    ax.scatter(X_test[X_test[:, 1]==j, 0], Y_pred[X_test[:, 1]==j],\n",
    "               label=f\"{encoder.classes_[j]} predict\",\n",
    "               color=f\"C{j}\",\n",
    "               marker=\"x\",\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142d176-9965-46bb-b984-728d3787cb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c43b1c5-fb8f-48a0-baca-247fb332d49a",
   "metadata": {
    "id": "9c43b1c5-fb8f-48a0-baca-247fb332d49a"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> Try the case where you don't provide the `species` feature to see how bad/good the fitting is (it's probably bad if `max_depth` is low).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try increasing `max_depth` and see how variability in data is recovered, and quantify errors accordingly as a function of `max_depth`. You may want to fix the `random_state` values.\n",
    ">\n",
    "> Just a note that the plotting of the tree may be slow with increasing `max_depth`; you probably can't see anything in that case anyway, so probaby just avoid that plot command.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Do the above but for `ccp_alpha` also. You may want to fix the `random_state` values.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> You can and should control over-fitting via other parameters that can be specified in the model initialisation, if one reason is to control the time it takes for the model to be trained. Have a look at what some of those are and try specifying those also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4c76f-f7cf-41f6-bb53-1012dd00792e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615f2410-0a38-40d5-a0c6-ebf9c51b705f",
   "metadata": {
    "id": "615f2410-0a38-40d5-a0c6-ebf9c51b705f"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Decision trees with dimension reduction approaches\n",
    "\n",
    "Consider doing a dimension reduction on the data first (e.g. PCA), and then fit a decision tree to that instead. Evaluate performance and dependence on parameters that control tree complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408926a-a573-4414-9687-d7de87f8e2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f28aa712-27ea-4224-86d5-f0635b162996",
   "metadata": {
    "id": "f28aa712-27ea-4224-86d5-f0635b162996"
   },
   "source": [
    "## 2) Classifying cats and dogs\n",
    "\n",
    "#### (This one is related to the upcoming assignment.)\n",
    "\n",
    "Try and do cats and dogs classification. You may or may not want to use dimension reduction approaches first.\n",
    "\n",
    "You may also want to revisit again after all the subsequent lectures (random forests, neural networks), because it's a hard test in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3dee6-469c-4b10-ab99-54e8af2c4897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d90c826-03a4-427f-9295-b553a1ad4a8a",
   "metadata": {
    "id": "4d90c826-03a4-427f-9295-b553a1ad4a8a"
   },
   "source": [
    "## 3) Predicting cats and dogs\n",
    "\n",
    "#### (This one is related to the upcoming assignment.)\n",
    "\n",
    "This one might be slow. Try and do cats and/or dog regression by predicting the right side of the face from the pixels on the left side, or top and bottom, or whatever really.\n",
    "\n",
    "Consider how you may couple in dimensional reduction techniques here as well, which will really help with model training speed (because otherwise your feature dimensions are very large).\n",
    "\n",
    "You may also want to revisit again after all the subsequent lectures (random forests, neural networks), because it's a hard test in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e281b-a16f-44a5-b14c-8d4f6ea8b05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5abea18c-ae1a-4a96-a77c-e594c7d6834e",
   "metadata": {
    "id": "5abea18c-ae1a-4a96-a77c-e594c7d6834e"
   },
   "source": [
    "## 4) Time series forecasting\n",
    "\n",
    "Consider regression on time series data like `elnino34_sst.data`, Lotka-Volterra, Lorenz, or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e7f50-f251-43f3-8926-737f911e40ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
